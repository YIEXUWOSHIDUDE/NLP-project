{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 2499,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020008003201280513,
      "grad_norm": 6.641031742095947,
      "learning_rate": 4.9999506126384855e-05,
      "loss": 2.029,
      "num_input_tokens_seen": 46592,
      "step": 5
    },
    {
      "epoch": 0.004001600640256103,
      "grad_norm": 1.9593167304992676,
      "learning_rate": 4.9998024525052316e-05,
      "loss": 1.0583,
      "num_input_tokens_seen": 94080,
      "step": 10
    },
    {
      "epoch": 0.006002400960384154,
      "grad_norm": 2.179232120513916,
      "learning_rate": 4.999555525454028e-05,
      "loss": 0.6575,
      "num_input_tokens_seen": 142336,
      "step": 15
    },
    {
      "epoch": 0.008003201280512205,
      "grad_norm": 1.9265774488449097,
      "learning_rate": 4.999209841240936e-05,
      "loss": 0.4031,
      "num_input_tokens_seen": 189376,
      "step": 20
    },
    {
      "epoch": 0.010004001600640256,
      "grad_norm": 2.287978410720825,
      "learning_rate": 4.9987654135239e-05,
      "loss": 0.279,
      "num_input_tokens_seen": 236416,
      "step": 25
    },
    {
      "epoch": 0.012004801920768308,
      "grad_norm": 0.8740359544754028,
      "learning_rate": 4.9982222598622095e-05,
      "loss": 0.1579,
      "num_input_tokens_seen": 285312,
      "step": 30
    },
    {
      "epoch": 0.014005602240896359,
      "grad_norm": 0.7307171821594238,
      "learning_rate": 4.997580401715806e-05,
      "loss": 0.1019,
      "num_input_tokens_seen": 330432,
      "step": 35
    },
    {
      "epoch": 0.01600640256102441,
      "grad_norm": 1.7551507949829102,
      "learning_rate": 4.9968398644444346e-05,
      "loss": 0.1373,
      "num_input_tokens_seen": 377920,
      "step": 40
    },
    {
      "epoch": 0.01800720288115246,
      "grad_norm": 1.7732598781585693,
      "learning_rate": 4.996000677306639e-05,
      "loss": 0.1209,
      "num_input_tokens_seen": 427200,
      "step": 45
    },
    {
      "epoch": 0.020008003201280513,
      "grad_norm": 0.5619576573371887,
      "learning_rate": 4.995062873458611e-05,
      "loss": 0.1243,
      "num_input_tokens_seen": 470272,
      "step": 50
    },
    {
      "epoch": 0.022008803521408563,
      "grad_norm": 1.065305471420288,
      "learning_rate": 4.994026489952878e-05,
      "loss": 0.0921,
      "num_input_tokens_seen": 517696,
      "step": 55
    },
    {
      "epoch": 0.024009603841536616,
      "grad_norm": 0.5516560077667236,
      "learning_rate": 4.9928915677368355e-05,
      "loss": 0.0536,
      "num_input_tokens_seen": 562048,
      "step": 60
    },
    {
      "epoch": 0.026010404161664665,
      "grad_norm": 0.8569363951683044,
      "learning_rate": 4.991658151651135e-05,
      "loss": 0.0731,
      "num_input_tokens_seen": 608640,
      "step": 65
    },
    {
      "epoch": 0.028011204481792718,
      "grad_norm": 1.2599552869796753,
      "learning_rate": 4.99032629042791e-05,
      "loss": 0.0745,
      "num_input_tokens_seen": 653824,
      "step": 70
    },
    {
      "epoch": 0.030012004801920768,
      "grad_norm": 1.0958391427993774,
      "learning_rate": 4.988896036688849e-05,
      "loss": 0.0839,
      "num_input_tokens_seen": 700480,
      "step": 75
    },
    {
      "epoch": 0.03201280512204882,
      "grad_norm": 0.5621402263641357,
      "learning_rate": 4.987367446943121e-05,
      "loss": 0.0608,
      "num_input_tokens_seen": 748736,
      "step": 80
    },
    {
      "epoch": 0.034013605442176874,
      "grad_norm": 0.9750357866287231,
      "learning_rate": 4.985740581585134e-05,
      "loss": 0.0719,
      "num_input_tokens_seen": 791552,
      "step": 85
    },
    {
      "epoch": 0.03601440576230492,
      "grad_norm": 1.005643606185913,
      "learning_rate": 4.984015504892161e-05,
      "loss": 0.0544,
      "num_input_tokens_seen": 837888,
      "step": 90
    },
    {
      "epoch": 0.03801520608243297,
      "grad_norm": 0.9017043113708496,
      "learning_rate": 4.98219228502179e-05,
      "loss": 0.0674,
      "num_input_tokens_seen": 883968,
      "step": 95
    },
    {
      "epoch": 0.040016006402561026,
      "grad_norm": 1.2920384407043457,
      "learning_rate": 4.9802709940092345e-05,
      "loss": 0.052,
      "num_input_tokens_seen": 929664,
      "step": 100
    },
    {
      "epoch": 0.04201680672268908,
      "grad_norm": 0.7276637554168701,
      "learning_rate": 4.978251707764492e-05,
      "loss": 0.0588,
      "num_input_tokens_seen": 975808,
      "step": 105
    },
    {
      "epoch": 0.044017607042817125,
      "grad_norm": 0.7785747647285461,
      "learning_rate": 4.976134506069338e-05,
      "loss": 0.0628,
      "num_input_tokens_seen": 1025984,
      "step": 110
    },
    {
      "epoch": 0.04601840736294518,
      "grad_norm": 0.9680784940719604,
      "learning_rate": 4.9739194725741756e-05,
      "loss": 0.0597,
      "num_input_tokens_seen": 1072896,
      "step": 115
    },
    {
      "epoch": 0.04801920768307323,
      "grad_norm": 0.5140121579170227,
      "learning_rate": 4.971606694794733e-05,
      "loss": 0.0657,
      "num_input_tokens_seen": 1119296,
      "step": 120
    },
    {
      "epoch": 0.05002000800320128,
      "grad_norm": 1.2241649627685547,
      "learning_rate": 4.9691962641086055e-05,
      "loss": 0.0648,
      "num_input_tokens_seen": 1164672,
      "step": 125
    },
    {
      "epoch": 0.05202080832332933,
      "grad_norm": 0.7189832329750061,
      "learning_rate": 4.9666882757516406e-05,
      "loss": 0.0517,
      "num_input_tokens_seen": 1211456,
      "step": 130
    },
    {
      "epoch": 0.05402160864345738,
      "grad_norm": 0.5243701934814453,
      "learning_rate": 4.9640828288141815e-05,
      "loss": 0.0736,
      "num_input_tokens_seen": 1257152,
      "step": 135
    },
    {
      "epoch": 0.056022408963585436,
      "grad_norm": 0.7248262166976929,
      "learning_rate": 4.961380026237148e-05,
      "loss": 0.0449,
      "num_input_tokens_seen": 1305600,
      "step": 140
    },
    {
      "epoch": 0.05802320928371348,
      "grad_norm": 0.8068388104438782,
      "learning_rate": 4.958579974807971e-05,
      "loss": 0.0556,
      "num_input_tokens_seen": 1353536,
      "step": 145
    },
    {
      "epoch": 0.060024009603841535,
      "grad_norm": 0.817277729511261,
      "learning_rate": 4.9556827851563706e-05,
      "loss": 0.0586,
      "num_input_tokens_seen": 1402432,
      "step": 150
    },
    {
      "epoch": 0.06202480992396959,
      "grad_norm": 1.2026060819625854,
      "learning_rate": 4.95268857174999e-05,
      "loss": 0.0947,
      "num_input_tokens_seen": 1450048,
      "step": 155
    },
    {
      "epoch": 0.06402561024409764,
      "grad_norm": 0.697123646736145,
      "learning_rate": 4.949597452889869e-05,
      "loss": 0.0532,
      "num_input_tokens_seen": 1495040,
      "step": 160
    },
    {
      "epoch": 0.06602641056422569,
      "grad_norm": 0.8072155117988586,
      "learning_rate": 4.946409550705772e-05,
      "loss": 0.0567,
      "num_input_tokens_seen": 1542592,
      "step": 165
    },
    {
      "epoch": 0.06802721088435375,
      "grad_norm": 1.2231212854385376,
      "learning_rate": 4.94312499115136e-05,
      "loss": 0.0699,
      "num_input_tokens_seen": 1591488,
      "step": 170
    },
    {
      "epoch": 0.0700280112044818,
      "grad_norm": 0.925482451915741,
      "learning_rate": 4.939743903999218e-05,
      "loss": 0.062,
      "num_input_tokens_seen": 1638080,
      "step": 175
    },
    {
      "epoch": 0.07202881152460984,
      "grad_norm": 0.6329382658004761,
      "learning_rate": 4.9362664228357246e-05,
      "loss": 0.0472,
      "num_input_tokens_seen": 1688128,
      "step": 180
    },
    {
      "epoch": 0.0740296118447379,
      "grad_norm": 1.0513843297958374,
      "learning_rate": 4.9326926850557744e-05,
      "loss": 0.0708,
      "num_input_tokens_seen": 1735104,
      "step": 185
    },
    {
      "epoch": 0.07603041216486595,
      "grad_norm": 0.7079252600669861,
      "learning_rate": 4.9290228318573524e-05,
      "loss": 0.0552,
      "num_input_tokens_seen": 1785216,
      "step": 190
    },
    {
      "epoch": 0.07803121248499399,
      "grad_norm": 0.837525486946106,
      "learning_rate": 4.925257008235951e-05,
      "loss": 0.0604,
      "num_input_tokens_seen": 1834560,
      "step": 195
    },
    {
      "epoch": 0.08003201280512205,
      "grad_norm": 0.9326125383377075,
      "learning_rate": 4.921395362978845e-05,
      "loss": 0.0643,
      "num_input_tokens_seen": 1880768,
      "step": 200
    },
    {
      "epoch": 0.0820328131252501,
      "grad_norm": 0.6402268409729004,
      "learning_rate": 4.9174380486592097e-05,
      "loss": 0.0669,
      "num_input_tokens_seen": 1927680,
      "step": 205
    },
    {
      "epoch": 0.08403361344537816,
      "grad_norm": 0.36337903141975403,
      "learning_rate": 4.9133852216300965e-05,
      "loss": 0.0487,
      "num_input_tokens_seen": 1973952,
      "step": 210
    },
    {
      "epoch": 0.0860344137655062,
      "grad_norm": 0.856375515460968,
      "learning_rate": 4.909237042018252e-05,
      "loss": 0.0617,
      "num_input_tokens_seen": 2020416,
      "step": 215
    },
    {
      "epoch": 0.08803521408563425,
      "grad_norm": 0.3990146815776825,
      "learning_rate": 4.904993673717793e-05,
      "loss": 0.0517,
      "num_input_tokens_seen": 2067840,
      "step": 220
    },
    {
      "epoch": 0.09003601440576231,
      "grad_norm": 0.799095869064331,
      "learning_rate": 4.9006552843837303e-05,
      "loss": 0.0355,
      "num_input_tokens_seen": 2114368,
      "step": 225
    },
    {
      "epoch": 0.09203681472589036,
      "grad_norm": 0.6904581189155579,
      "learning_rate": 4.896222045425347e-05,
      "loss": 0.0661,
      "num_input_tokens_seen": 2159168,
      "step": 230
    },
    {
      "epoch": 0.0940376150460184,
      "grad_norm": 0.8897150754928589,
      "learning_rate": 4.891694131999423e-05,
      "loss": 0.044,
      "num_input_tokens_seen": 2206016,
      "step": 235
    },
    {
      "epoch": 0.09603841536614646,
      "grad_norm": 0.775146484375,
      "learning_rate": 4.8870717230033155e-05,
      "loss": 0.0559,
      "num_input_tokens_seen": 2253568,
      "step": 240
    },
    {
      "epoch": 0.09803921568627451,
      "grad_norm": 0.5047109127044678,
      "learning_rate": 4.882355001067892e-05,
      "loss": 0.0594,
      "num_input_tokens_seen": 2299200,
      "step": 245
    },
    {
      "epoch": 0.10004001600640255,
      "grad_norm": 0.7179999351501465,
      "learning_rate": 4.877544152550313e-05,
      "loss": 0.0718,
      "num_input_tokens_seen": 2346752,
      "step": 250
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 0.6242130398750305,
      "learning_rate": 4.8726393675266716e-05,
      "loss": 0.0649,
      "num_input_tokens_seen": 2392320,
      "step": 255
    },
    {
      "epoch": 0.10404161664665866,
      "grad_norm": 0.6024152636528015,
      "learning_rate": 4.867640839784481e-05,
      "loss": 0.0533,
      "num_input_tokens_seen": 2440832,
      "step": 260
    },
    {
      "epoch": 0.10604241696678672,
      "grad_norm": 0.680256187915802,
      "learning_rate": 4.862548766815017e-05,
      "loss": 0.0471,
      "num_input_tokens_seen": 2485440,
      "step": 265
    },
    {
      "epoch": 0.10804321728691477,
      "grad_norm": 0.9025201797485352,
      "learning_rate": 4.857363349805519e-05,
      "loss": 0.0509,
      "num_input_tokens_seen": 2530944,
      "step": 270
    },
    {
      "epoch": 0.11004401760704281,
      "grad_norm": 0.7306554317474365,
      "learning_rate": 4.852084793631239e-05,
      "loss": 0.0392,
      "num_input_tokens_seen": 2577984,
      "step": 275
    },
    {
      "epoch": 0.11204481792717087,
      "grad_norm": 0.5365092754364014,
      "learning_rate": 4.846713306847347e-05,
      "loss": 0.0537,
      "num_input_tokens_seen": 2622656,
      "step": 280
    },
    {
      "epoch": 0.11404561824729892,
      "grad_norm": 0.8942270278930664,
      "learning_rate": 4.8412491016806895e-05,
      "loss": 0.0504,
      "num_input_tokens_seen": 2669760,
      "step": 285
    },
    {
      "epoch": 0.11604641856742696,
      "grad_norm": 0.21313296258449554,
      "learning_rate": 4.835692394021408e-05,
      "loss": 0.0477,
      "num_input_tokens_seen": 2717568,
      "step": 290
    },
    {
      "epoch": 0.11804721888755502,
      "grad_norm": 0.7463487386703491,
      "learning_rate": 4.830043403414406e-05,
      "loss": 0.0404,
      "num_input_tokens_seen": 2761920,
      "step": 295
    },
    {
      "epoch": 0.12004801920768307,
      "grad_norm": 0.8419709205627441,
      "learning_rate": 4.824302353050678e-05,
      "loss": 0.063,
      "num_input_tokens_seen": 2806272,
      "step": 300
    },
    {
      "epoch": 0.12204881952781113,
      "grad_norm": 0.795359194278717,
      "learning_rate": 4.818469469758486e-05,
      "loss": 0.046,
      "num_input_tokens_seen": 2847872,
      "step": 305
    },
    {
      "epoch": 0.12404961984793918,
      "grad_norm": 0.9155678153038025,
      "learning_rate": 4.812544983994404e-05,
      "loss": 0.0492,
      "num_input_tokens_seen": 2892288,
      "step": 310
    },
    {
      "epoch": 0.12605042016806722,
      "grad_norm": 0.8210747838020325,
      "learning_rate": 4.806529129834208e-05,
      "loss": 0.0519,
      "num_input_tokens_seen": 2936256,
      "step": 315
    },
    {
      "epoch": 0.12805122048819528,
      "grad_norm": 0.49533453583717346,
      "learning_rate": 4.800422144963628e-05,
      "loss": 0.0345,
      "num_input_tokens_seen": 2982144,
      "step": 320
    },
    {
      "epoch": 0.13005202080832334,
      "grad_norm": 0.4187827408313751,
      "learning_rate": 4.794224270668961e-05,
      "loss": 0.0539,
      "num_input_tokens_seen": 3025920,
      "step": 325
    },
    {
      "epoch": 0.13205282112845138,
      "grad_norm": 0.32874542474746704,
      "learning_rate": 4.7879357518275334e-05,
      "loss": 0.0266,
      "num_input_tokens_seen": 3075136,
      "step": 330
    },
    {
      "epoch": 0.13405362144857944,
      "grad_norm": 0.8246257305145264,
      "learning_rate": 4.781556836898028e-05,
      "loss": 0.0607,
      "num_input_tokens_seen": 3123520,
      "step": 335
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 0.6850496530532837,
      "learning_rate": 4.7750877779106666e-05,
      "loss": 0.0595,
      "num_input_tokens_seen": 3170240,
      "step": 340
    },
    {
      "epoch": 0.13805522208883553,
      "grad_norm": 0.6295422911643982,
      "learning_rate": 4.768528830457254e-05,
      "loss": 0.0341,
      "num_input_tokens_seen": 3217152,
      "step": 345
    },
    {
      "epoch": 0.1400560224089636,
      "grad_norm": 0.6021708250045776,
      "learning_rate": 4.761880253681076e-05,
      "loss": 0.0512,
      "num_input_tokens_seen": 3265472,
      "step": 350
    },
    {
      "epoch": 0.14205682272909165,
      "grad_norm": 0.6271921396255493,
      "learning_rate": 4.755142310266666e-05,
      "loss": 0.0515,
      "num_input_tokens_seen": 3312320,
      "step": 355
    },
    {
      "epoch": 0.14405762304921968,
      "grad_norm": 1.2649927139282227,
      "learning_rate": 4.74831526642942e-05,
      "loss": 0.0416,
      "num_input_tokens_seen": 3358336,
      "step": 360
    },
    {
      "epoch": 0.14605842336934774,
      "grad_norm": 0.6481446027755737,
      "learning_rate": 4.741399391905086e-05,
      "loss": 0.0325,
      "num_input_tokens_seen": 3404480,
      "step": 365
    },
    {
      "epoch": 0.1480592236894758,
      "grad_norm": 0.5920732617378235,
      "learning_rate": 4.734394959939098e-05,
      "loss": 0.0487,
      "num_input_tokens_seen": 3449024,
      "step": 370
    },
    {
      "epoch": 0.15006002400960383,
      "grad_norm": 0.5229991674423218,
      "learning_rate": 4.727302247275789e-05,
      "loss": 0.0429,
      "num_input_tokens_seen": 3495168,
      "step": 375
    },
    {
      "epoch": 0.1520608243297319,
      "grad_norm": 0.56439608335495,
      "learning_rate": 4.720121534147449e-05,
      "loss": 0.0351,
      "num_input_tokens_seen": 3542848,
      "step": 380
    },
    {
      "epoch": 0.15406162464985995,
      "grad_norm": 0.6275750398635864,
      "learning_rate": 4.712853104263258e-05,
      "loss": 0.043,
      "num_input_tokens_seen": 3589504,
      "step": 385
    },
    {
      "epoch": 0.15606242496998798,
      "grad_norm": 0.5651528239250183,
      "learning_rate": 4.705497244798076e-05,
      "loss": 0.0406,
      "num_input_tokens_seen": 3637056,
      "step": 390
    },
    {
      "epoch": 0.15806322529011604,
      "grad_norm": 0.4617699086666107,
      "learning_rate": 4.6980542463810966e-05,
      "loss": 0.0519,
      "num_input_tokens_seen": 3682368,
      "step": 395
    },
    {
      "epoch": 0.1600640256102441,
      "grad_norm": 0.4417508840560913,
      "learning_rate": 4.690524403084361e-05,
      "loss": 0.0473,
      "num_input_tokens_seen": 3731712,
      "step": 400
    },
    {
      "epoch": 0.16206482593037214,
      "grad_norm": 0.7644271850585938,
      "learning_rate": 4.682908012411145e-05,
      "loss": 0.0299,
      "num_input_tokens_seen": 3778624,
      "step": 405
    },
    {
      "epoch": 0.1640656262505002,
      "grad_norm": 0.5996727347373962,
      "learning_rate": 4.675205375284199e-05,
      "loss": 0.057,
      "num_input_tokens_seen": 3822656,
      "step": 410
    },
    {
      "epoch": 0.16606642657062826,
      "grad_norm": 0.7119103074073792,
      "learning_rate": 4.667416796033863e-05,
      "loss": 0.0598,
      "num_input_tokens_seen": 3868864,
      "step": 415
    },
    {
      "epoch": 0.16806722689075632,
      "grad_norm": 0.6133835911750793,
      "learning_rate": 4.659542582386041e-05,
      "loss": 0.0344,
      "num_input_tokens_seen": 3917760,
      "step": 420
    },
    {
      "epoch": 0.17006802721088435,
      "grad_norm": 0.6313204765319824,
      "learning_rate": 4.651583045450041e-05,
      "loss": 0.048,
      "num_input_tokens_seen": 3965696,
      "step": 425
    },
    {
      "epoch": 0.1720688275310124,
      "grad_norm": 1.1253098249435425,
      "learning_rate": 4.643538499706286e-05,
      "loss": 0.0589,
      "num_input_tokens_seen": 4012864,
      "step": 430
    },
    {
      "epoch": 0.17406962785114047,
      "grad_norm": 0.5415818691253662,
      "learning_rate": 4.635409262993886e-05,
      "loss": 0.0475,
      "num_input_tokens_seen": 4057920,
      "step": 435
    },
    {
      "epoch": 0.1760704281712685,
      "grad_norm": 0.7430975437164307,
      "learning_rate": 4.627195656498084e-05,
      "loss": 0.0484,
      "num_input_tokens_seen": 4105984,
      "step": 440
    },
    {
      "epoch": 0.17807122849139656,
      "grad_norm": 0.4851268529891968,
      "learning_rate": 4.618898004737564e-05,
      "loss": 0.0307,
      "num_input_tokens_seen": 4152448,
      "step": 445
    },
    {
      "epoch": 0.18007202881152462,
      "grad_norm": 0.5365416407585144,
      "learning_rate": 4.610516635551625e-05,
      "loss": 0.0434,
      "num_input_tokens_seen": 4197440,
      "step": 450
    },
    {
      "epoch": 0.18207282913165265,
      "grad_norm": 0.4408954083919525,
      "learning_rate": 4.6020518800872356e-05,
      "loss": 0.0522,
      "num_input_tokens_seen": 4244736,
      "step": 455
    },
    {
      "epoch": 0.1840736294517807,
      "grad_norm": 0.857399046421051,
      "learning_rate": 4.593504072785948e-05,
      "loss": 0.0593,
      "num_input_tokens_seen": 4292352,
      "step": 460
    },
    {
      "epoch": 0.18607442977190877,
      "grad_norm": 0.30126503109931946,
      "learning_rate": 4.58487355137068e-05,
      "loss": 0.0591,
      "num_input_tokens_seen": 4342144,
      "step": 465
    },
    {
      "epoch": 0.1880752300920368,
      "grad_norm": 0.350417822599411,
      "learning_rate": 4.576160656832378e-05,
      "loss": 0.0435,
      "num_input_tokens_seen": 4388864,
      "step": 470
    },
    {
      "epoch": 0.19007603041216486,
      "grad_norm": 0.6261410117149353,
      "learning_rate": 4.5673657334165386e-05,
      "loss": 0.0414,
      "num_input_tokens_seen": 4436096,
      "step": 475
    },
    {
      "epoch": 0.19207683073229292,
      "grad_norm": 0.4362337589263916,
      "learning_rate": 4.558489128609612e-05,
      "loss": 0.0471,
      "num_input_tokens_seen": 4480384,
      "step": 480
    },
    {
      "epoch": 0.19407763105242096,
      "grad_norm": 0.526943027973175,
      "learning_rate": 4.5495311931252716e-05,
      "loss": 0.0393,
      "num_input_tokens_seen": 4528576,
      "step": 485
    },
    {
      "epoch": 0.19607843137254902,
      "grad_norm": 0.38203319907188416,
      "learning_rate": 4.540492280890555e-05,
      "loss": 0.0441,
      "num_input_tokens_seen": 4575424,
      "step": 490
    },
    {
      "epoch": 0.19807923169267708,
      "grad_norm": 0.46990644931793213,
      "learning_rate": 4.5313727490318825e-05,
      "loss": 0.0353,
      "num_input_tokens_seen": 4625024,
      "step": 495
    },
    {
      "epoch": 0.2000800320128051,
      "grad_norm": 0.48101478815078735,
      "learning_rate": 4.522172957860949e-05,
      "loss": 0.038,
      "num_input_tokens_seen": 4673600,
      "step": 500
    },
    {
      "epoch": 0.20208083233293317,
      "grad_norm": 0.31921079754829407,
      "learning_rate": 4.5128932708604835e-05,
      "loss": 0.037,
      "num_input_tokens_seen": 4722560,
      "step": 505
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 0.5705456733703613,
      "learning_rate": 4.503534054669892e-05,
      "loss": 0.0379,
      "num_input_tokens_seen": 4768896,
      "step": 510
    },
    {
      "epoch": 0.2060824329731893,
      "grad_norm": 0.5312667489051819,
      "learning_rate": 4.494095679070769e-05,
      "loss": 0.0359,
      "num_input_tokens_seen": 4817280,
      "step": 515
    },
    {
      "epoch": 0.20808323329331732,
      "grad_norm": 0.5106739401817322,
      "learning_rate": 4.484578516972288e-05,
      "loss": 0.0303,
      "num_input_tokens_seen": 4861888,
      "step": 520
    },
    {
      "epoch": 0.21008403361344538,
      "grad_norm": 0.7853392362594604,
      "learning_rate": 4.4749829443964705e-05,
      "loss": 0.0466,
      "num_input_tokens_seen": 4910400,
      "step": 525
    },
    {
      "epoch": 0.21208483393357344,
      "grad_norm": 0.44977930188179016,
      "learning_rate": 4.4653093404633245e-05,
      "loss": 0.0281,
      "num_input_tokens_seen": 4959808,
      "step": 530
    },
    {
      "epoch": 0.21408563425370147,
      "grad_norm": 0.46352308988571167,
      "learning_rate": 4.455558087375871e-05,
      "loss": 0.0413,
      "num_input_tokens_seen": 5004864,
      "step": 535
    },
    {
      "epoch": 0.21608643457382953,
      "grad_norm": 0.6556841135025024,
      "learning_rate": 4.4457295704050376e-05,
      "loss": 0.0392,
      "num_input_tokens_seen": 5053312,
      "step": 540
    },
    {
      "epoch": 0.2180872348939576,
      "grad_norm": 0.5904219150543213,
      "learning_rate": 4.435824177874442e-05,
      "loss": 0.0422,
      "num_input_tokens_seen": 5101632,
      "step": 545
    },
    {
      "epoch": 0.22008803521408563,
      "grad_norm": 0.6900328397750854,
      "learning_rate": 4.425842301145047e-05,
      "loss": 0.0385,
      "num_input_tokens_seen": 5150784,
      "step": 550
    },
    {
      "epoch": 0.22208883553421369,
      "grad_norm": 0.5656455159187317,
      "learning_rate": 4.415784334599693e-05,
      "loss": 0.0537,
      "num_input_tokens_seen": 5199104,
      "step": 555
    },
    {
      "epoch": 0.22408963585434175,
      "grad_norm": 0.4275628924369812,
      "learning_rate": 4.405650675627526e-05,
      "loss": 0.0455,
      "num_input_tokens_seen": 5244352,
      "step": 560
    },
    {
      "epoch": 0.22609043617446978,
      "grad_norm": 0.7231613397598267,
      "learning_rate": 4.39544172460829e-05,
      "loss": 0.0561,
      "num_input_tokens_seen": 5293120,
      "step": 565
    },
    {
      "epoch": 0.22809123649459784,
      "grad_norm": 0.4486784040927887,
      "learning_rate": 4.3851578848965075e-05,
      "loss": 0.0492,
      "num_input_tokens_seen": 5341056,
      "step": 570
    },
    {
      "epoch": 0.2300920368147259,
      "grad_norm": 0.5405669212341309,
      "learning_rate": 4.374799562805546e-05,
      "loss": 0.0554,
      "num_input_tokens_seen": 5386880,
      "step": 575
    },
    {
      "epoch": 0.23209283713485393,
      "grad_norm": 0.7877563238143921,
      "learning_rate": 4.364367167591564e-05,
      "loss": 0.0373,
      "num_input_tokens_seen": 5433408,
      "step": 580
    },
    {
      "epoch": 0.234093637454982,
      "grad_norm": 0.36934947967529297,
      "learning_rate": 4.3538611114373416e-05,
      "loss": 0.0362,
      "num_input_tokens_seen": 5479680,
      "step": 585
    },
    {
      "epoch": 0.23609443777511005,
      "grad_norm": 0.6631835103034973,
      "learning_rate": 4.3432818094359915e-05,
      "loss": 0.0443,
      "num_input_tokens_seen": 5525184,
      "step": 590
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.5625964403152466,
      "learning_rate": 4.332629679574566e-05,
      "loss": 0.0587,
      "num_input_tokens_seen": 5572544,
      "step": 595
    },
    {
      "epoch": 0.24009603841536614,
      "grad_norm": 0.5824211835861206,
      "learning_rate": 4.3219051427175344e-05,
      "loss": 0.0405,
      "num_input_tokens_seen": 5620480,
      "step": 600
    },
    {
      "epoch": 0.2420968387354942,
      "grad_norm": 0.41979095339775085,
      "learning_rate": 4.3111086225901596e-05,
      "loss": 0.0395,
      "num_input_tokens_seen": 5667712,
      "step": 605
    },
    {
      "epoch": 0.24409763905562226,
      "grad_norm": 0.4882581830024719,
      "learning_rate": 4.3002405457617567e-05,
      "loss": 0.0485,
      "num_input_tokens_seen": 5716480,
      "step": 610
    },
    {
      "epoch": 0.2460984393757503,
      "grad_norm": 0.5516102313995361,
      "learning_rate": 4.289301341628836e-05,
      "loss": 0.0463,
      "num_input_tokens_seen": 5762240,
      "step": 615
    },
    {
      "epoch": 0.24809923969587835,
      "grad_norm": 0.7317147850990295,
      "learning_rate": 4.2782914423981425e-05,
      "loss": 0.0391,
      "num_input_tokens_seen": 5809728,
      "step": 620
    },
    {
      "epoch": 0.2501000400160064,
      "grad_norm": 0.49464815855026245,
      "learning_rate": 4.267211283069573e-05,
      "loss": 0.0327,
      "num_input_tokens_seen": 5857792,
      "step": 625
    },
    {
      "epoch": 0.25210084033613445,
      "grad_norm": 0.5099672675132751,
      "learning_rate": 4.2560613014189966e-05,
      "loss": 0.0469,
      "num_input_tokens_seen": 5906944,
      "step": 630
    },
    {
      "epoch": 0.2541016406562625,
      "grad_norm": 0.6903344392776489,
      "learning_rate": 4.2448419379809516e-05,
      "loss": 0.0458,
      "num_input_tokens_seen": 5954112,
      "step": 635
    },
    {
      "epoch": 0.25610244097639057,
      "grad_norm": 0.4576149880886078,
      "learning_rate": 4.233553636031246e-05,
      "loss": 0.033,
      "num_input_tokens_seen": 6001216,
      "step": 640
    },
    {
      "epoch": 0.2581032412965186,
      "grad_norm": 0.7954393625259399,
      "learning_rate": 4.222196841569438e-05,
      "loss": 0.0475,
      "num_input_tokens_seen": 6050304,
      "step": 645
    },
    {
      "epoch": 0.2601040416166467,
      "grad_norm": 0.780372679233551,
      "learning_rate": 4.21077200330122e-05,
      "loss": 0.0476,
      "num_input_tokens_seen": 6095552,
      "step": 650
    },
    {
      "epoch": 0.2621048419367747,
      "grad_norm": 0.3450513184070587,
      "learning_rate": 4.199279572620684e-05,
      "loss": 0.048,
      "num_input_tokens_seen": 6139776,
      "step": 655
    },
    {
      "epoch": 0.26410564225690275,
      "grad_norm": 0.24030530452728271,
      "learning_rate": 4.187720003592496e-05,
      "loss": 0.0349,
      "num_input_tokens_seen": 6184576,
      "step": 660
    },
    {
      "epoch": 0.2661064425770308,
      "grad_norm": 0.5419843196868896,
      "learning_rate": 4.176093752933945e-05,
      "loss": 0.0355,
      "num_input_tokens_seen": 6231296,
      "step": 665
    },
    {
      "epoch": 0.26810724289715887,
      "grad_norm": 0.6790710091590881,
      "learning_rate": 4.164401279996907e-05,
      "loss": 0.0443,
      "num_input_tokens_seen": 6278080,
      "step": 670
    },
    {
      "epoch": 0.27010804321728693,
      "grad_norm": 0.42421334981918335,
      "learning_rate": 4.152643046749693e-05,
      "loss": 0.0329,
      "num_input_tokens_seen": 6324416,
      "step": 675
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 0.3721616864204407,
      "learning_rate": 4.140819517758795e-05,
      "loss": 0.0376,
      "num_input_tokens_seen": 6367040,
      "step": 680
    },
    {
      "epoch": 0.274109643857543,
      "grad_norm": 0.6132134199142456,
      "learning_rate": 4.128931160170536e-05,
      "loss": 0.0449,
      "num_input_tokens_seen": 6414976,
      "step": 685
    },
    {
      "epoch": 0.27611044417767105,
      "grad_norm": 0.3782403767108917,
      "learning_rate": 4.116978443692604e-05,
      "loss": 0.0415,
      "num_input_tokens_seen": 6465600,
      "step": 690
    },
    {
      "epoch": 0.2781112444977991,
      "grad_norm": 0.37936505675315857,
      "learning_rate": 4.104961840575505e-05,
      "loss": 0.0348,
      "num_input_tokens_seen": 6515584,
      "step": 695
    },
    {
      "epoch": 0.2801120448179272,
      "grad_norm": 0.8555272817611694,
      "learning_rate": 4.092881825593895e-05,
      "loss": 0.0593,
      "num_input_tokens_seen": 6560576,
      "step": 700
    },
    {
      "epoch": 0.28211284513805523,
      "grad_norm": 0.6273022294044495,
      "learning_rate": 4.08073887602783e-05,
      "loss": 0.0453,
      "num_input_tokens_seen": 6605696,
      "step": 705
    },
    {
      "epoch": 0.2841136454581833,
      "grad_norm": 0.3006141483783722,
      "learning_rate": 4.0685334716438994e-05,
      "loss": 0.0441,
      "num_input_tokens_seen": 6651584,
      "step": 710
    },
    {
      "epoch": 0.2861144457783113,
      "grad_norm": 0.5265094637870789,
      "learning_rate": 4.0562660946762804e-05,
      "loss": 0.0374,
      "num_input_tokens_seen": 6697344,
      "step": 715
    },
    {
      "epoch": 0.28811524609843936,
      "grad_norm": 0.673646867275238,
      "learning_rate": 4.0439372298076764e-05,
      "loss": 0.044,
      "num_input_tokens_seen": 6746368,
      "step": 720
    },
    {
      "epoch": 0.2901160464185674,
      "grad_norm": 0.4433079957962036,
      "learning_rate": 4.0315473641501734e-05,
      "loss": 0.0341,
      "num_input_tokens_seen": 6795136,
      "step": 725
    },
    {
      "epoch": 0.2921168467386955,
      "grad_norm": 0.3832692503929138,
      "learning_rate": 4.019096987225991e-05,
      "loss": 0.0568,
      "num_input_tokens_seen": 6839104,
      "step": 730
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.7608329653739929,
      "learning_rate": 4.0065865909481417e-05,
      "loss": 0.0486,
      "num_input_tokens_seen": 6886400,
      "step": 735
    },
    {
      "epoch": 0.2961184473789516,
      "grad_norm": 0.5089850425720215,
      "learning_rate": 3.994016669600995e-05,
      "loss": 0.0509,
      "num_input_tokens_seen": 6936576,
      "step": 740
    },
    {
      "epoch": 0.29811924769907966,
      "grad_norm": 0.8730225563049316,
      "learning_rate": 3.981387719820754e-05,
      "loss": 0.0331,
      "num_input_tokens_seen": 6984960,
      "step": 745
    },
    {
      "epoch": 0.30012004801920766,
      "grad_norm": 0.5602078437805176,
      "learning_rate": 3.9687002405758225e-05,
      "loss": 0.0453,
      "num_input_tokens_seen": 7033344,
      "step": 750
    },
    {
      "epoch": 0.3021208483393357,
      "grad_norm": 0.4045090079307556,
      "learning_rate": 3.955954733147101e-05,
      "loss": 0.0375,
      "num_input_tokens_seen": 7081344,
      "step": 755
    },
    {
      "epoch": 0.3041216486594638,
      "grad_norm": 0.5617544651031494,
      "learning_rate": 3.9431517011081756e-05,
      "loss": 0.0448,
      "num_input_tokens_seen": 7129216,
      "step": 760
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 0.5347396731376648,
      "learning_rate": 3.9302916503054246e-05,
      "loss": 0.0389,
      "num_input_tokens_seen": 7174720,
      "step": 765
    },
    {
      "epoch": 0.3081232492997199,
      "grad_norm": 0.4436207413673401,
      "learning_rate": 3.917375088838029e-05,
      "loss": 0.047,
      "num_input_tokens_seen": 7222784,
      "step": 770
    },
    {
      "epoch": 0.31012404961984796,
      "grad_norm": 0.7381861209869385,
      "learning_rate": 3.9044025270379025e-05,
      "loss": 0.0512,
      "num_input_tokens_seen": 7271424,
      "step": 775
    },
    {
      "epoch": 0.31212484993997597,
      "grad_norm": 0.7652621865272522,
      "learning_rate": 3.891374477449525e-05,
      "loss": 0.0418,
      "num_input_tokens_seen": 7321152,
      "step": 780
    },
    {
      "epoch": 0.31412565026010403,
      "grad_norm": 0.5644971132278442,
      "learning_rate": 3.87829145480969e-05,
      "loss": 0.0411,
      "num_input_tokens_seen": 7367936,
      "step": 785
    },
    {
      "epoch": 0.3161264505802321,
      "grad_norm": 0.7252211570739746,
      "learning_rate": 3.865153976027176e-05,
      "loss": 0.035,
      "num_input_tokens_seen": 7416512,
      "step": 790
    },
    {
      "epoch": 0.31812725090036015,
      "grad_norm": 0.515780508518219,
      "learning_rate": 3.851962560162312e-05,
      "loss": 0.0394,
      "num_input_tokens_seen": 7465728,
      "step": 795
    },
    {
      "epoch": 0.3201280512204882,
      "grad_norm": 0.4764760434627533,
      "learning_rate": 3.8387177284064765e-05,
      "loss": 0.0282,
      "num_input_tokens_seen": 7512704,
      "step": 800
    },
    {
      "epoch": 0.32212885154061627,
      "grad_norm": 0.48902323842048645,
      "learning_rate": 3.825420004061507e-05,
      "loss": 0.0422,
      "num_input_tokens_seen": 7559616,
      "step": 805
    },
    {
      "epoch": 0.3241296518607443,
      "grad_norm": 0.5668327212333679,
      "learning_rate": 3.8120699125190195e-05,
      "loss": 0.0429,
      "num_input_tokens_seen": 7607040,
      "step": 810
    },
    {
      "epoch": 0.32613045218087233,
      "grad_norm": 0.7100394368171692,
      "learning_rate": 3.798667981239649e-05,
      "loss": 0.0388,
      "num_input_tokens_seen": 7652992,
      "step": 815
    },
    {
      "epoch": 0.3281312525010004,
      "grad_norm": 0.3528212904930115,
      "learning_rate": 3.785214739732218e-05,
      "loss": 0.0178,
      "num_input_tokens_seen": 7699136,
      "step": 820
    },
    {
      "epoch": 0.33013205282112845,
      "grad_norm": 0.43826669454574585,
      "learning_rate": 3.771710719532806e-05,
      "loss": 0.028,
      "num_input_tokens_seen": 7746176,
      "step": 825
    },
    {
      "epoch": 0.3321328531412565,
      "grad_norm": 0.423921674489975,
      "learning_rate": 3.7581564541837565e-05,
      "loss": 0.0414,
      "num_input_tokens_seen": 7793088,
      "step": 830
    },
    {
      "epoch": 0.33413365346138457,
      "grad_norm": 0.4426652193069458,
      "learning_rate": 3.744552479212592e-05,
      "loss": 0.0499,
      "num_input_tokens_seen": 7841408,
      "step": 835
    },
    {
      "epoch": 0.33613445378151263,
      "grad_norm": 0.4179009795188904,
      "learning_rate": 3.7308993321108556e-05,
      "loss": 0.0315,
      "num_input_tokens_seen": 7887424,
      "step": 840
    },
    {
      "epoch": 0.33813525410164064,
      "grad_norm": 0.41237786412239075,
      "learning_rate": 3.717197552312877e-05,
      "loss": 0.0283,
      "num_input_tokens_seen": 7932288,
      "step": 845
    },
    {
      "epoch": 0.3401360544217687,
      "grad_norm": 0.56982421875,
      "learning_rate": 3.703447681174458e-05,
      "loss": 0.0472,
      "num_input_tokens_seen": 7980992,
      "step": 850
    },
    {
      "epoch": 0.34213685474189676,
      "grad_norm": 0.6874497532844543,
      "learning_rate": 3.6896502619514836e-05,
      "loss": 0.0222,
      "num_input_tokens_seen": 8028480,
      "step": 855
    },
    {
      "epoch": 0.3441376550620248,
      "grad_norm": 0.5120871663093567,
      "learning_rate": 3.675805839778459e-05,
      "loss": 0.0288,
      "num_input_tokens_seen": 8076352,
      "step": 860
    },
    {
      "epoch": 0.3461384553821529,
      "grad_norm": 0.33841603994369507,
      "learning_rate": 3.66191496164697e-05,
      "loss": 0.0319,
      "num_input_tokens_seen": 8122048,
      "step": 865
    },
    {
      "epoch": 0.34813925570228094,
      "grad_norm": 0.4948175847530365,
      "learning_rate": 3.6479781763840736e-05,
      "loss": 0.0349,
      "num_input_tokens_seen": 8167680,
      "step": 870
    },
    {
      "epoch": 0.35014005602240894,
      "grad_norm": 0.9089634418487549,
      "learning_rate": 3.6339960346306105e-05,
      "loss": 0.0503,
      "num_input_tokens_seen": 8215488,
      "step": 875
    },
    {
      "epoch": 0.352140856342537,
      "grad_norm": 0.4323318898677826,
      "learning_rate": 3.619969088819454e-05,
      "loss": 0.0316,
      "num_input_tokens_seen": 8262016,
      "step": 880
    },
    {
      "epoch": 0.35414165666266506,
      "grad_norm": 0.8570324778556824,
      "learning_rate": 3.6058978931536764e-05,
      "loss": 0.0469,
      "num_input_tokens_seen": 8309248,
      "step": 885
    },
    {
      "epoch": 0.3561424569827931,
      "grad_norm": 0.33783042430877686,
      "learning_rate": 3.5917830035846616e-05,
      "loss": 0.0472,
      "num_input_tokens_seen": 8359104,
      "step": 890
    },
    {
      "epoch": 0.3581432573029212,
      "grad_norm": 0.748363196849823,
      "learning_rate": 3.577624977790132e-05,
      "loss": 0.0349,
      "num_input_tokens_seen": 8409088,
      "step": 895
    },
    {
      "epoch": 0.36014405762304924,
      "grad_norm": 0.5663020610809326,
      "learning_rate": 3.563424375152118e-05,
      "loss": 0.0425,
      "num_input_tokens_seen": 8454400,
      "step": 900
    },
    {
      "epoch": 0.36214485794317725,
      "grad_norm": 0.45558491349220276,
      "learning_rate": 3.549181756734858e-05,
      "loss": 0.0373,
      "num_input_tokens_seen": 8500160,
      "step": 905
    },
    {
      "epoch": 0.3641456582633053,
      "grad_norm": 0.40994951128959656,
      "learning_rate": 3.5348976852626256e-05,
      "loss": 0.0296,
      "num_input_tokens_seen": 8548352,
      "step": 910
    },
    {
      "epoch": 0.36614645858343337,
      "grad_norm": 0.40434151887893677,
      "learning_rate": 3.520572725097504e-05,
      "loss": 0.0282,
      "num_input_tokens_seen": 8593920,
      "step": 915
    },
    {
      "epoch": 0.3681472589035614,
      "grad_norm": 1.0496023893356323,
      "learning_rate": 3.506207442217081e-05,
      "loss": 0.039,
      "num_input_tokens_seen": 8640320,
      "step": 920
    },
    {
      "epoch": 0.3701480592236895,
      "grad_norm": 0.5849403142929077,
      "learning_rate": 3.491802404192092e-05,
      "loss": 0.0449,
      "num_input_tokens_seen": 8687360,
      "step": 925
    },
    {
      "epoch": 0.37214885954381755,
      "grad_norm": 0.3039001524448395,
      "learning_rate": 3.477358180163994e-05,
      "loss": 0.0283,
      "num_input_tokens_seen": 8734336,
      "step": 930
    },
    {
      "epoch": 0.3741496598639456,
      "grad_norm": 0.3899812400341034,
      "learning_rate": 3.4628753408224765e-05,
      "loss": 0.0438,
      "num_input_tokens_seen": 8778560,
      "step": 935
    },
    {
      "epoch": 0.3761504601840736,
      "grad_norm": 0.5757461786270142,
      "learning_rate": 3.4483544583829205e-05,
      "loss": 0.0357,
      "num_input_tokens_seen": 8823360,
      "step": 940
    },
    {
      "epoch": 0.37815126050420167,
      "grad_norm": 0.7681429386138916,
      "learning_rate": 3.433796106563779e-05,
      "loss": 0.0413,
      "num_input_tokens_seen": 8870976,
      "step": 945
    },
    {
      "epoch": 0.38015206082432973,
      "grad_norm": 0.4102286398410797,
      "learning_rate": 3.419200860563922e-05,
      "loss": 0.031,
      "num_input_tokens_seen": 8916416,
      "step": 950
    },
    {
      "epoch": 0.3821528611444578,
      "grad_norm": 0.9361910223960876,
      "learning_rate": 3.4045692970399e-05,
      "loss": 0.0579,
      "num_input_tokens_seen": 8962752,
      "step": 955
    },
    {
      "epoch": 0.38415366146458585,
      "grad_norm": 0.24588049948215485,
      "learning_rate": 3.389901994083168e-05,
      "loss": 0.0427,
      "num_input_tokens_seen": 9008000,
      "step": 960
    },
    {
      "epoch": 0.3861544617847139,
      "grad_norm": 0.3531439006328583,
      "learning_rate": 3.375199531197241e-05,
      "loss": 0.0378,
      "num_input_tokens_seen": 9054464,
      "step": 965
    },
    {
      "epoch": 0.3881552621048419,
      "grad_norm": 0.2388022392988205,
      "learning_rate": 3.3604624892747985e-05,
      "loss": 0.0408,
      "num_input_tokens_seen": 9100480,
      "step": 970
    },
    {
      "epoch": 0.39015606242497,
      "grad_norm": 0.4813693165779114,
      "learning_rate": 3.345691450574733e-05,
      "loss": 0.0312,
      "num_input_tokens_seen": 9147136,
      "step": 975
    },
    {
      "epoch": 0.39215686274509803,
      "grad_norm": 0.41873612999916077,
      "learning_rate": 3.330886998699149e-05,
      "loss": 0.0339,
      "num_input_tokens_seen": 9196672,
      "step": 980
    },
    {
      "epoch": 0.3941576630652261,
      "grad_norm": 0.4924736022949219,
      "learning_rate": 3.3160497185702996e-05,
      "loss": 0.0486,
      "num_input_tokens_seen": 9244288,
      "step": 985
    },
    {
      "epoch": 0.39615846338535415,
      "grad_norm": 0.687720537185669,
      "learning_rate": 3.301180196407477e-05,
      "loss": 0.0363,
      "num_input_tokens_seen": 9292160,
      "step": 990
    },
    {
      "epoch": 0.3981592637054822,
      "grad_norm": 0.7048586010932922,
      "learning_rate": 3.2862790197038565e-05,
      "loss": 0.0379,
      "num_input_tokens_seen": 9338944,
      "step": 995
    },
    {
      "epoch": 0.4001600640256102,
      "grad_norm": 0.2938198745250702,
      "learning_rate": 3.271346777203279e-05,
      "loss": 0.0302,
      "num_input_tokens_seen": 9386048,
      "step": 1000
    },
    {
      "epoch": 0.4021608643457383,
      "grad_norm": 0.4594199061393738,
      "learning_rate": 3.2563840588769895e-05,
      "loss": 0.0391,
      "num_input_tokens_seen": 9431744,
      "step": 1005
    },
    {
      "epoch": 0.40416166466586634,
      "grad_norm": 0.28596264123916626,
      "learning_rate": 3.241391455900332e-05,
      "loss": 0.019,
      "num_input_tokens_seen": 9478912,
      "step": 1010
    },
    {
      "epoch": 0.4061624649859944,
      "grad_norm": 0.8993960618972778,
      "learning_rate": 3.2263695606293905e-05,
      "loss": 0.0388,
      "num_input_tokens_seen": 9523008,
      "step": 1015
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.5174104571342468,
      "learning_rate": 3.211318966577581e-05,
      "loss": 0.047,
      "num_input_tokens_seen": 9570048,
      "step": 1020
    },
    {
      "epoch": 0.4101640656262505,
      "grad_norm": 0.20359811186790466,
      "learning_rate": 3.1962402683922086e-05,
      "loss": 0.0324,
      "num_input_tokens_seen": 9618240,
      "step": 1025
    },
    {
      "epoch": 0.4121648659463786,
      "grad_norm": 0.2841545045375824,
      "learning_rate": 3.181134061830967e-05,
      "loss": 0.0372,
      "num_input_tokens_seen": 9662848,
      "step": 1030
    },
    {
      "epoch": 0.4141656662665066,
      "grad_norm": 0.20213717222213745,
      "learning_rate": 3.166000943738405e-05,
      "loss": 0.0251,
      "num_input_tokens_seen": 9709312,
      "step": 1035
    },
    {
      "epoch": 0.41616646658663464,
      "grad_norm": 0.08484037965536118,
      "learning_rate": 3.1508415120223404e-05,
      "loss": 0.0368,
      "num_input_tokens_seen": 9753408,
      "step": 1040
    },
    {
      "epoch": 0.4181672669067627,
      "grad_norm": 0.53019118309021,
      "learning_rate": 3.1356563656302415e-05,
      "loss": 0.052,
      "num_input_tokens_seen": 9800576,
      "step": 1045
    },
    {
      "epoch": 0.42016806722689076,
      "grad_norm": 0.35223960876464844,
      "learning_rate": 3.1204461045255604e-05,
      "loss": 0.0271,
      "num_input_tokens_seen": 9850176,
      "step": 1050
    },
    {
      "epoch": 0.4221688675470188,
      "grad_norm": 0.5439430475234985,
      "learning_rate": 3.1052113296640265e-05,
      "loss": 0.0379,
      "num_input_tokens_seen": 9898816,
      "step": 1055
    },
    {
      "epoch": 0.4241696678671469,
      "grad_norm": 0.40553271770477295,
      "learning_rate": 3.089952642969909e-05,
      "loss": 0.0389,
      "num_input_tokens_seen": 9945536,
      "step": 1060
    },
    {
      "epoch": 0.4261704681872749,
      "grad_norm": 0.5128524303436279,
      "learning_rate": 3.074670647312228e-05,
      "loss": 0.0328,
      "num_input_tokens_seen": 9990144,
      "step": 1065
    },
    {
      "epoch": 0.42817126850740295,
      "grad_norm": 0.7292575836181641,
      "learning_rate": 3.0593659464809377e-05,
      "loss": 0.0378,
      "num_input_tokens_seen": 10040384,
      "step": 1070
    },
    {
      "epoch": 0.430172068827531,
      "grad_norm": 0.6707523465156555,
      "learning_rate": 3.0440391451630733e-05,
      "loss": 0.0244,
      "num_input_tokens_seen": 10086720,
      "step": 1075
    },
    {
      "epoch": 0.43217286914765907,
      "grad_norm": 0.563775897026062,
      "learning_rate": 3.0286908489188576e-05,
      "loss": 0.0426,
      "num_input_tokens_seen": 10134784,
      "step": 1080
    },
    {
      "epoch": 0.4341736694677871,
      "grad_norm": 0.4511346220970154,
      "learning_rate": 3.0133216641577732e-05,
      "loss": 0.0355,
      "num_input_tokens_seen": 10179840,
      "step": 1085
    },
    {
      "epoch": 0.4361744697879152,
      "grad_norm": 0.6180378794670105,
      "learning_rate": 2.997932198114608e-05,
      "loss": 0.0352,
      "num_input_tokens_seen": 10224000,
      "step": 1090
    },
    {
      "epoch": 0.4381752701080432,
      "grad_norm": 0.6409558653831482,
      "learning_rate": 2.9825230588254616e-05,
      "loss": 0.0406,
      "num_input_tokens_seen": 10266944,
      "step": 1095
    },
    {
      "epoch": 0.44017607042817125,
      "grad_norm": 0.3551771342754364,
      "learning_rate": 2.9670948551037174e-05,
      "loss": 0.0237,
      "num_input_tokens_seen": 10316160,
      "step": 1100
    },
    {
      "epoch": 0.4421768707482993,
      "grad_norm": 0.4525127112865448,
      "learning_rate": 2.9516481965159975e-05,
      "loss": 0.0332,
      "num_input_tokens_seen": 10364416,
      "step": 1105
    },
    {
      "epoch": 0.44417767106842737,
      "grad_norm": 0.8135414123535156,
      "learning_rate": 2.9361836933580706e-05,
      "loss": 0.0305,
      "num_input_tokens_seen": 10409280,
      "step": 1110
    },
    {
      "epoch": 0.44617847138855543,
      "grad_norm": 0.5953177213668823,
      "learning_rate": 2.920701956630743e-05,
      "loss": 0.0369,
      "num_input_tokens_seen": 10458944,
      "step": 1115
    },
    {
      "epoch": 0.4481792717086835,
      "grad_norm": 0.3641238808631897,
      "learning_rate": 2.9052035980157183e-05,
      "loss": 0.023,
      "num_input_tokens_seen": 10507648,
      "step": 1120
    },
    {
      "epoch": 0.45018007202881155,
      "grad_norm": 0.6275926232337952,
      "learning_rate": 2.8896892298514278e-05,
      "loss": 0.0393,
      "num_input_tokens_seen": 10555648,
      "step": 1125
    },
    {
      "epoch": 0.45218087234893956,
      "grad_norm": 0.47814756631851196,
      "learning_rate": 2.874159465108839e-05,
      "loss": 0.0257,
      "num_input_tokens_seen": 10601856,
      "step": 1130
    },
    {
      "epoch": 0.4541816726690676,
      "grad_norm": 0.5528836250305176,
      "learning_rate": 2.858614917367236e-05,
      "loss": 0.0324,
      "num_input_tokens_seen": 10647360,
      "step": 1135
    },
    {
      "epoch": 0.4561824729891957,
      "grad_norm": 0.6047577857971191,
      "learning_rate": 2.843056200789978e-05,
      "loss": 0.048,
      "num_input_tokens_seen": 10697984,
      "step": 1140
    },
    {
      "epoch": 0.45818327330932374,
      "grad_norm": 0.5904996395111084,
      "learning_rate": 2.827483930100234e-05,
      "loss": 0.0318,
      "num_input_tokens_seen": 10750208,
      "step": 1145
    },
    {
      "epoch": 0.4601840736294518,
      "grad_norm": 0.5287331938743591,
      "learning_rate": 2.8118987205566928e-05,
      "loss": 0.0376,
      "num_input_tokens_seen": 10797248,
      "step": 1150
    },
    {
      "epoch": 0.46218487394957986,
      "grad_norm": 0.4727523922920227,
      "learning_rate": 2.7963011879292573e-05,
      "loss": 0.0322,
      "num_input_tokens_seen": 10844288,
      "step": 1155
    },
    {
      "epoch": 0.46418567426970786,
      "grad_norm": 0.3579913079738617,
      "learning_rate": 2.780691948474713e-05,
      "loss": 0.0232,
      "num_input_tokens_seen": 10894144,
      "step": 1160
    },
    {
      "epoch": 0.4661864745898359,
      "grad_norm": 0.5067083239555359,
      "learning_rate": 2.7650716189123822e-05,
      "loss": 0.026,
      "num_input_tokens_seen": 10937664,
      "step": 1165
    },
    {
      "epoch": 0.468187274909964,
      "grad_norm": 0.5413468480110168,
      "learning_rate": 2.7494408163997553e-05,
      "loss": 0.0372,
      "num_input_tokens_seen": 10985280,
      "step": 1170
    },
    {
      "epoch": 0.47018807523009204,
      "grad_norm": 0.6622741222381592,
      "learning_rate": 2.7338001585081074e-05,
      "loss": 0.0415,
      "num_input_tokens_seen": 11031168,
      "step": 1175
    },
    {
      "epoch": 0.4721888755502201,
      "grad_norm": 0.48955896496772766,
      "learning_rate": 2.718150263198099e-05,
      "loss": 0.042,
      "num_input_tokens_seen": 11078272,
      "step": 1180
    },
    {
      "epoch": 0.47418967587034816,
      "grad_norm": 0.4429726004600525,
      "learning_rate": 2.7024917487953606e-05,
      "loss": 0.0305,
      "num_input_tokens_seen": 11126144,
      "step": 1185
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.46919339895248413,
      "learning_rate": 2.686825233966061e-05,
      "loss": 0.0226,
      "num_input_tokens_seen": 11171840,
      "step": 1190
    },
    {
      "epoch": 0.4781912765106042,
      "grad_norm": 0.44548970460891724,
      "learning_rate": 2.6711513376924653e-05,
      "loss": 0.0263,
      "num_input_tokens_seen": 11217984,
      "step": 1195
    },
    {
      "epoch": 0.4801920768307323,
      "grad_norm": 0.32691338658332825,
      "learning_rate": 2.655470679248479e-05,
      "loss": 0.0244,
      "num_input_tokens_seen": 11265344,
      "step": 1200
    },
    {
      "epoch": 0.48219287715086034,
      "grad_norm": 0.4280213415622711,
      "learning_rate": 2.63978387817518e-05,
      "loss": 0.0276,
      "num_input_tokens_seen": 11310208,
      "step": 1205
    },
    {
      "epoch": 0.4841936774709884,
      "grad_norm": 0.7646196484565735,
      "learning_rate": 2.6240915542563406e-05,
      "loss": 0.0467,
      "num_input_tokens_seen": 11356224,
      "step": 1210
    },
    {
      "epoch": 0.48619447779111646,
      "grad_norm": 0.467641144990921,
      "learning_rate": 2.6083943274939404e-05,
      "loss": 0.0271,
      "num_input_tokens_seen": 11400768,
      "step": 1215
    },
    {
      "epoch": 0.4881952781112445,
      "grad_norm": 1.0442837476730347,
      "learning_rate": 2.5926928180836697e-05,
      "loss": 0.0405,
      "num_input_tokens_seen": 11447360,
      "step": 1220
    },
    {
      "epoch": 0.49019607843137253,
      "grad_norm": 0.5132151246070862,
      "learning_rate": 2.5769876463904265e-05,
      "loss": 0.0367,
      "num_input_tokens_seen": 11493632,
      "step": 1225
    },
    {
      "epoch": 0.4921968787515006,
      "grad_norm": 0.5105052590370178,
      "learning_rate": 2.5612794329238034e-05,
      "loss": 0.038,
      "num_input_tokens_seen": 11540096,
      "step": 1230
    },
    {
      "epoch": 0.49419767907162865,
      "grad_norm": 0.44103682041168213,
      "learning_rate": 2.5455687983135738e-05,
      "loss": 0.0415,
      "num_input_tokens_seen": 11586496,
      "step": 1235
    },
    {
      "epoch": 0.4961984793917567,
      "grad_norm": 0.46231788396835327,
      "learning_rate": 2.529856363285172e-05,
      "loss": 0.0344,
      "num_input_tokens_seen": 11632896,
      "step": 1240
    },
    {
      "epoch": 0.49819927971188477,
      "grad_norm": 0.5977881550788879,
      "learning_rate": 2.5141427486351644e-05,
      "loss": 0.0262,
      "num_input_tokens_seen": 11678464,
      "step": 1245
    },
    {
      "epoch": 0.5002000800320128,
      "grad_norm": 0.508668065071106,
      "learning_rate": 2.498428575206725e-05,
      "loss": 0.0346,
      "num_input_tokens_seen": 11725056,
      "step": 1250
    },
    {
      "epoch": 0.5022008803521408,
      "grad_norm": 0.6192441582679749,
      "learning_rate": 2.4827144638651053e-05,
      "loss": 0.0451,
      "num_input_tokens_seen": 11776704,
      "step": 1255
    },
    {
      "epoch": 0.5042016806722689,
      "grad_norm": 0.3420757055282593,
      "learning_rate": 2.467001035473103e-05,
      "loss": 0.019,
      "num_input_tokens_seen": 11822912,
      "step": 1260
    },
    {
      "epoch": 0.506202480992397,
      "grad_norm": 0.488114595413208,
      "learning_rate": 2.4512889108665332e-05,
      "loss": 0.0325,
      "num_input_tokens_seen": 11870464,
      "step": 1265
    },
    {
      "epoch": 0.508203281312525,
      "grad_norm": 0.4357965886592865,
      "learning_rate": 2.4355787108296987e-05,
      "loss": 0.0346,
      "num_input_tokens_seen": 11917888,
      "step": 1270
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 0.5022966861724854,
      "learning_rate": 2.419871056070862e-05,
      "loss": 0.0231,
      "num_input_tokens_seen": 11966144,
      "step": 1275
    },
    {
      "epoch": 0.5122048819527811,
      "grad_norm": 0.6617288589477539,
      "learning_rate": 2.4041665671977226e-05,
      "loss": 0.0514,
      "num_input_tokens_seen": 12017728,
      "step": 1280
    },
    {
      "epoch": 0.5142056822729092,
      "grad_norm": 0.6155290007591248,
      "learning_rate": 2.3884658646928963e-05,
      "loss": 0.0359,
      "num_input_tokens_seen": 12064640,
      "step": 1285
    },
    {
      "epoch": 0.5162064825930373,
      "grad_norm": 0.5610056519508362,
      "learning_rate": 2.372769568889399e-05,
      "loss": 0.0377,
      "num_input_tokens_seen": 12110528,
      "step": 1290
    },
    {
      "epoch": 0.5182072829131653,
      "grad_norm": 0.909713625907898,
      "learning_rate": 2.357078299946139e-05,
      "loss": 0.0392,
      "num_input_tokens_seen": 12155392,
      "step": 1295
    },
    {
      "epoch": 0.5202080832332934,
      "grad_norm": 0.5387253761291504,
      "learning_rate": 2.3413926778234144e-05,
      "loss": 0.0348,
      "num_input_tokens_seen": 12202176,
      "step": 1300
    },
    {
      "epoch": 0.5222088835534213,
      "grad_norm": 0.558292806148529,
      "learning_rate": 2.3257133222584183e-05,
      "loss": 0.0427,
      "num_input_tokens_seen": 12248256,
      "step": 1305
    },
    {
      "epoch": 0.5242096838735494,
      "grad_norm": 0.5056742429733276,
      "learning_rate": 2.3100408527407492e-05,
      "loss": 0.0335,
      "num_input_tokens_seen": 12293824,
      "step": 1310
    },
    {
      "epoch": 0.5262104841936774,
      "grad_norm": 0.5001261830329895,
      "learning_rate": 2.2943758884879434e-05,
      "loss": 0.0306,
      "num_input_tokens_seen": 12340672,
      "step": 1315
    },
    {
      "epoch": 0.5282112845138055,
      "grad_norm": 0.23411142826080322,
      "learning_rate": 2.2787190484210027e-05,
      "loss": 0.029,
      "num_input_tokens_seen": 12387904,
      "step": 1320
    },
    {
      "epoch": 0.5302120848339336,
      "grad_norm": 0.224275603890419,
      "learning_rate": 2.2630709511399436e-05,
      "loss": 0.0367,
      "num_input_tokens_seen": 12433856,
      "step": 1325
    },
    {
      "epoch": 0.5322128851540616,
      "grad_norm": 0.2213643342256546,
      "learning_rate": 2.247432214899356e-05,
      "loss": 0.0246,
      "num_input_tokens_seen": 12481984,
      "step": 1330
    },
    {
      "epoch": 0.5342136854741897,
      "grad_norm": 0.5662004351615906,
      "learning_rate": 2.231803457583976e-05,
      "loss": 0.0312,
      "num_input_tokens_seen": 12528320,
      "step": 1335
    },
    {
      "epoch": 0.5362144857943177,
      "grad_norm": 0.4769577383995056,
      "learning_rate": 2.2161852966842736e-05,
      "loss": 0.0385,
      "num_input_tokens_seen": 12573504,
      "step": 1340
    },
    {
      "epoch": 0.5382152861144458,
      "grad_norm": 0.5108867883682251,
      "learning_rate": 2.200578349272056e-05,
      "loss": 0.0243,
      "num_input_tokens_seen": 12619456,
      "step": 1345
    },
    {
      "epoch": 0.5402160864345739,
      "grad_norm": 0.34682512283325195,
      "learning_rate": 2.184983231976086e-05,
      "loss": 0.0285,
      "num_input_tokens_seen": 12667008,
      "step": 1350
    },
    {
      "epoch": 0.5422168867547019,
      "grad_norm": 0.6768253445625305,
      "learning_rate": 2.1694005609577204e-05,
      "loss": 0.0324,
      "num_input_tokens_seen": 12713600,
      "step": 1355
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 0.7514706254005432,
      "learning_rate": 2.1538309518865646e-05,
      "loss": 0.0312,
      "num_input_tokens_seen": 12759168,
      "step": 1360
    },
    {
      "epoch": 0.5462184873949579,
      "grad_norm": 0.38931915163993835,
      "learning_rate": 2.1382750199161496e-05,
      "loss": 0.0425,
      "num_input_tokens_seen": 12808896,
      "step": 1365
    },
    {
      "epoch": 0.548219287715086,
      "grad_norm": 0.559042751789093,
      "learning_rate": 2.1227333796596217e-05,
      "loss": 0.027,
      "num_input_tokens_seen": 12855616,
      "step": 1370
    },
    {
      "epoch": 0.550220088035214,
      "grad_norm": 0.4722348749637604,
      "learning_rate": 2.107206645165467e-05,
      "loss": 0.0331,
      "num_input_tokens_seen": 12902912,
      "step": 1375
    },
    {
      "epoch": 0.5522208883553421,
      "grad_norm": 0.25074201822280884,
      "learning_rate": 2.0916954298932446e-05,
      "loss": 0.0286,
      "num_input_tokens_seen": 12951168,
      "step": 1380
    },
    {
      "epoch": 0.5542216886754702,
      "grad_norm": 0.36967965960502625,
      "learning_rate": 2.0762003466893516e-05,
      "loss": 0.035,
      "num_input_tokens_seen": 12996480,
      "step": 1385
    },
    {
      "epoch": 0.5562224889955982,
      "grad_norm": 0.5563114881515503,
      "learning_rate": 2.0607220077628086e-05,
      "loss": 0.0349,
      "num_input_tokens_seen": 13040576,
      "step": 1390
    },
    {
      "epoch": 0.5582232893157263,
      "grad_norm": 0.7938106656074524,
      "learning_rate": 2.0452610246610724e-05,
      "loss": 0.0344,
      "num_input_tokens_seen": 13088128,
      "step": 1395
    },
    {
      "epoch": 0.5602240896358543,
      "grad_norm": 0.4620434641838074,
      "learning_rate": 2.029818008245872e-05,
      "loss": 0.0457,
      "num_input_tokens_seen": 13135104,
      "step": 1400
    },
    {
      "epoch": 0.5622248899559824,
      "grad_norm": 0.38327765464782715,
      "learning_rate": 2.0143935686690746e-05,
      "loss": 0.0317,
      "num_input_tokens_seen": 13182272,
      "step": 1405
    },
    {
      "epoch": 0.5642256902761105,
      "grad_norm": 0.5004881024360657,
      "learning_rate": 1.99898831534858e-05,
      "loss": 0.0299,
      "num_input_tokens_seen": 13228544,
      "step": 1410
    },
    {
      "epoch": 0.5662264905962385,
      "grad_norm": 0.3832392394542694,
      "learning_rate": 1.9836028569442393e-05,
      "loss": 0.0233,
      "num_input_tokens_seen": 13274496,
      "step": 1415
    },
    {
      "epoch": 0.5682272909163666,
      "grad_norm": 0.523277223110199,
      "learning_rate": 1.9682378013338105e-05,
      "loss": 0.03,
      "num_input_tokens_seen": 13318336,
      "step": 1420
    },
    {
      "epoch": 0.5702280912364946,
      "grad_norm": 0.34714096784591675,
      "learning_rate": 1.9528937555889373e-05,
      "loss": 0.0264,
      "num_input_tokens_seen": 13365120,
      "step": 1425
    },
    {
      "epoch": 0.5722288915566226,
      "grad_norm": 0.5240742564201355,
      "learning_rate": 1.9375713259511685e-05,
      "loss": 0.04,
      "num_input_tokens_seen": 13410240,
      "step": 1430
    },
    {
      "epoch": 0.5742296918767507,
      "grad_norm": 0.5013653635978699,
      "learning_rate": 1.9222711178080002e-05,
      "loss": 0.0281,
      "num_input_tokens_seen": 13455744,
      "step": 1435
    },
    {
      "epoch": 0.5762304921968787,
      "grad_norm": 0.6465976238250732,
      "learning_rate": 1.9069937356689616e-05,
      "loss": 0.0294,
      "num_input_tokens_seen": 13501184,
      "step": 1440
    },
    {
      "epoch": 0.5782312925170068,
      "grad_norm": 0.4748724102973938,
      "learning_rate": 1.8917397831417286e-05,
      "loss": 0.0346,
      "num_input_tokens_seen": 13549760,
      "step": 1445
    },
    {
      "epoch": 0.5802320928371348,
      "grad_norm": 0.4580073654651642,
      "learning_rate": 1.8765098629082753e-05,
      "loss": 0.0285,
      "num_input_tokens_seen": 13598400,
      "step": 1450
    },
    {
      "epoch": 0.5822328931572629,
      "grad_norm": 0.7271133065223694,
      "learning_rate": 1.861304576701063e-05,
      "loss": 0.0321,
      "num_input_tokens_seen": 13646016,
      "step": 1455
    },
    {
      "epoch": 0.584233693477391,
      "grad_norm": 0.6995546817779541,
      "learning_rate": 1.846124525279265e-05,
      "loss": 0.0406,
      "num_input_tokens_seen": 13690624,
      "step": 1460
    },
    {
      "epoch": 0.586234493797519,
      "grad_norm": 0.345672607421875,
      "learning_rate": 1.8309703084050324e-05,
      "loss": 0.0272,
      "num_input_tokens_seen": 13737472,
      "step": 1465
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.42978495359420776,
      "learning_rate": 1.815842524819793e-05,
      "loss": 0.0269,
      "num_input_tokens_seen": 13784832,
      "step": 1470
    },
    {
      "epoch": 0.5902360944377751,
      "grad_norm": 0.7338714599609375,
      "learning_rate": 1.8007417722206013e-05,
      "loss": 0.0318,
      "num_input_tokens_seen": 13832064,
      "step": 1475
    },
    {
      "epoch": 0.5922368947579032,
      "grad_norm": 0.22815988957881927,
      "learning_rate": 1.78566864723652e-05,
      "loss": 0.0344,
      "num_input_tokens_seen": 13876800,
      "step": 1480
    },
    {
      "epoch": 0.5942376950780313,
      "grad_norm": 0.7139140367507935,
      "learning_rate": 1.7706237454050457e-05,
      "loss": 0.0357,
      "num_input_tokens_seen": 13920832,
      "step": 1485
    },
    {
      "epoch": 0.5962384953981593,
      "grad_norm": 0.4759114682674408,
      "learning_rate": 1.7556076611485848e-05,
      "loss": 0.0251,
      "num_input_tokens_seen": 13969024,
      "step": 1490
    },
    {
      "epoch": 0.5982392957182873,
      "grad_norm": 0.45851024985313416,
      "learning_rate": 1.7406209877509627e-05,
      "loss": 0.025,
      "num_input_tokens_seen": 14017024,
      "step": 1495
    },
    {
      "epoch": 0.6002400960384153,
      "grad_norm": 0.503831148147583,
      "learning_rate": 1.7256643173339832e-05,
      "loss": 0.0238,
      "num_input_tokens_seen": 14062720,
      "step": 1500
    },
    {
      "epoch": 0.6022408963585434,
      "grad_norm": 0.3688637912273407,
      "learning_rate": 1.7107382408340383e-05,
      "loss": 0.0256,
      "num_input_tokens_seen": 14110720,
      "step": 1505
    },
    {
      "epoch": 0.6042416966786714,
      "grad_norm": 0.5270248055458069,
      "learning_rate": 1.6958433479787566e-05,
      "loss": 0.0442,
      "num_input_tokens_seen": 14156096,
      "step": 1510
    },
    {
      "epoch": 0.6062424969987995,
      "grad_norm": 0.5202706456184387,
      "learning_rate": 1.6809802272637054e-05,
      "loss": 0.0424,
      "num_input_tokens_seen": 14202496,
      "step": 1515
    },
    {
      "epoch": 0.6082432973189276,
      "grad_norm": 0.3108539283275604,
      "learning_rate": 1.666149465929137e-05,
      "loss": 0.0288,
      "num_input_tokens_seen": 14250240,
      "step": 1520
    },
    {
      "epoch": 0.6102440976390556,
      "grad_norm": 0.5230541229248047,
      "learning_rate": 1.651351649936789e-05,
      "loss": 0.0286,
      "num_input_tokens_seen": 14296512,
      "step": 1525
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 0.44011688232421875,
      "learning_rate": 1.6365873639467315e-05,
      "loss": 0.035,
      "num_input_tokens_seen": 14347968,
      "step": 1530
    },
    {
      "epoch": 0.6142456982793117,
      "grad_norm": 0.4476195275783539,
      "learning_rate": 1.6218571912942683e-05,
      "loss": 0.0221,
      "num_input_tokens_seen": 14392832,
      "step": 1535
    },
    {
      "epoch": 0.6162464985994398,
      "grad_norm": 0.653590738773346,
      "learning_rate": 1.6071617139668882e-05,
      "loss": 0.0304,
      "num_input_tokens_seen": 14441216,
      "step": 1540
    },
    {
      "epoch": 0.6182472989195679,
      "grad_norm": 0.3700268268585205,
      "learning_rate": 1.5925015125812736e-05,
      "loss": 0.028,
      "num_input_tokens_seen": 14488896,
      "step": 1545
    },
    {
      "epoch": 0.6202480992396959,
      "grad_norm": 0.3883724808692932,
      "learning_rate": 1.577877166360357e-05,
      "loss": 0.0354,
      "num_input_tokens_seen": 14536128,
      "step": 1550
    },
    {
      "epoch": 0.6222488995598239,
      "grad_norm": 0.430971622467041,
      "learning_rate": 1.5632892531104375e-05,
      "loss": 0.032,
      "num_input_tokens_seen": 14585728,
      "step": 1555
    },
    {
      "epoch": 0.6242496998799519,
      "grad_norm": 0.7159159779548645,
      "learning_rate": 1.5487383491983502e-05,
      "loss": 0.0255,
      "num_input_tokens_seen": 14631552,
      "step": 1560
    },
    {
      "epoch": 0.62625050020008,
      "grad_norm": 0.40050429105758667,
      "learning_rate": 1.534225029528697e-05,
      "loss": 0.0281,
      "num_input_tokens_seen": 14677312,
      "step": 1565
    },
    {
      "epoch": 0.6282513005202081,
      "grad_norm": 0.4188845753669739,
      "learning_rate": 1.5197498675211309e-05,
      "loss": 0.027,
      "num_input_tokens_seen": 14723648,
      "step": 1570
    },
    {
      "epoch": 0.6302521008403361,
      "grad_norm": 0.6959806680679321,
      "learning_rate": 1.5053134350876983e-05,
      "loss": 0.0344,
      "num_input_tokens_seen": 14768192,
      "step": 1575
    },
    {
      "epoch": 0.6322529011604642,
      "grad_norm": 0.46467819809913635,
      "learning_rate": 1.4909163026102457e-05,
      "loss": 0.0268,
      "num_input_tokens_seen": 14813760,
      "step": 1580
    },
    {
      "epoch": 0.6342537014805922,
      "grad_norm": 0.541536271572113,
      "learning_rate": 1.476559038917882e-05,
      "loss": 0.0245,
      "num_input_tokens_seen": 14861120,
      "step": 1585
    },
    {
      "epoch": 0.6362545018007203,
      "grad_norm": 0.5894545316696167,
      "learning_rate": 1.4622422112645054e-05,
      "loss": 0.0449,
      "num_input_tokens_seen": 14906880,
      "step": 1590
    },
    {
      "epoch": 0.6382553021208484,
      "grad_norm": 0.4771844446659088,
      "learning_rate": 1.4479663853063902e-05,
      "loss": 0.0246,
      "num_input_tokens_seen": 14953472,
      "step": 1595
    },
    {
      "epoch": 0.6402561024409764,
      "grad_norm": 0.3765035569667816,
      "learning_rate": 1.433732125079838e-05,
      "loss": 0.031,
      "num_input_tokens_seen": 15001280,
      "step": 1600
    },
    {
      "epoch": 0.6422569027611045,
      "grad_norm": 0.44290244579315186,
      "learning_rate": 1.4195399929788944e-05,
      "loss": 0.0191,
      "num_input_tokens_seen": 15050368,
      "step": 1605
    },
    {
      "epoch": 0.6442577030812325,
      "grad_norm": 0.4725460708141327,
      "learning_rate": 1.405390549733125e-05,
      "loss": 0.0294,
      "num_input_tokens_seen": 15095488,
      "step": 1610
    },
    {
      "epoch": 0.6462585034013606,
      "grad_norm": 0.5187258124351501,
      "learning_rate": 1.3912843543854664e-05,
      "loss": 0.031,
      "num_input_tokens_seen": 15141056,
      "step": 1615
    },
    {
      "epoch": 0.6482593037214885,
      "grad_norm": 0.43158158659935,
      "learning_rate": 1.3772219642701335e-05,
      "loss": 0.0206,
      "num_input_tokens_seen": 15189888,
      "step": 1620
    },
    {
      "epoch": 0.6502601040416166,
      "grad_norm": 0.5731756687164307,
      "learning_rate": 1.363203934990601e-05,
      "loss": 0.0276,
      "num_input_tokens_seen": 15237632,
      "step": 1625
    },
    {
      "epoch": 0.6522609043617447,
      "grad_norm": 0.9693593382835388,
      "learning_rate": 1.3492308203976523e-05,
      "loss": 0.0408,
      "num_input_tokens_seen": 15285248,
      "step": 1630
    },
    {
      "epoch": 0.6542617046818727,
      "grad_norm": 0.5956272482872009,
      "learning_rate": 1.3353031725674987e-05,
      "loss": 0.0247,
      "num_input_tokens_seen": 15332032,
      "step": 1635
    },
    {
      "epoch": 0.6562625050020008,
      "grad_norm": 0.441171795129776,
      "learning_rate": 1.3214215417799613e-05,
      "loss": 0.0275,
      "num_input_tokens_seen": 15377664,
      "step": 1640
    },
    {
      "epoch": 0.6582633053221288,
      "grad_norm": 0.5510262846946716,
      "learning_rate": 1.307586476496736e-05,
      "loss": 0.0232,
      "num_input_tokens_seen": 15422976,
      "step": 1645
    },
    {
      "epoch": 0.6602641056422569,
      "grad_norm": 0.7464564442634583,
      "learning_rate": 1.2937985233397179e-05,
      "loss": 0.0301,
      "num_input_tokens_seen": 15470976,
      "step": 1650
    },
    {
      "epoch": 0.662264905962385,
      "grad_norm": 0.35735321044921875,
      "learning_rate": 1.2800582270694106e-05,
      "loss": 0.0366,
      "num_input_tokens_seen": 15515072,
      "step": 1655
    },
    {
      "epoch": 0.664265706282513,
      "grad_norm": 0.2511870265007019,
      "learning_rate": 1.266366130563395e-05,
      "loss": 0.0292,
      "num_input_tokens_seen": 15561408,
      "step": 1660
    },
    {
      "epoch": 0.6662665066026411,
      "grad_norm": 0.2982253432273865,
      "learning_rate": 1.2527227747948895e-05,
      "loss": 0.0377,
      "num_input_tokens_seen": 15608320,
      "step": 1665
    },
    {
      "epoch": 0.6682673069227691,
      "grad_norm": 0.4621371924877167,
      "learning_rate": 1.239128698811367e-05,
      "loss": 0.0329,
      "num_input_tokens_seen": 15657664,
      "step": 1670
    },
    {
      "epoch": 0.6702681072428972,
      "grad_norm": 0.4153377413749695,
      "learning_rate": 1.2255844397132657e-05,
      "loss": 0.0394,
      "num_input_tokens_seen": 15707520,
      "step": 1675
    },
    {
      "epoch": 0.6722689075630253,
      "grad_norm": 0.5608718395233154,
      "learning_rate": 1.2120905326327598e-05,
      "loss": 0.0345,
      "num_input_tokens_seen": 15753792,
      "step": 1680
    },
    {
      "epoch": 0.6742697078831532,
      "grad_norm": 0.49138784408569336,
      "learning_rate": 1.1986475107126249e-05,
      "loss": 0.0242,
      "num_input_tokens_seen": 15803840,
      "step": 1685
    },
    {
      "epoch": 0.6762705082032813,
      "grad_norm": 0.5318578481674194,
      "learning_rate": 1.1852559050851669e-05,
      "loss": 0.0343,
      "num_input_tokens_seen": 15849344,
      "step": 1690
    },
    {
      "epoch": 0.6782713085234093,
      "grad_norm": 0.5873845815658569,
      "learning_rate": 1.17191624485124e-05,
      "loss": 0.0253,
      "num_input_tokens_seen": 15896640,
      "step": 1695
    },
    {
      "epoch": 0.6802721088435374,
      "grad_norm": 0.42427706718444824,
      "learning_rate": 1.1586290570593434e-05,
      "loss": 0.0207,
      "num_input_tokens_seen": 15942016,
      "step": 1700
    },
    {
      "epoch": 0.6822729091636655,
      "grad_norm": 0.44330233335494995,
      "learning_rate": 1.1453948666847928e-05,
      "loss": 0.0401,
      "num_input_tokens_seen": 15987840,
      "step": 1705
    },
    {
      "epoch": 0.6842737094837935,
      "grad_norm": 0.459616482257843,
      "learning_rate": 1.132214196608986e-05,
      "loss": 0.0279,
      "num_input_tokens_seen": 16033792,
      "step": 1710
    },
    {
      "epoch": 0.6862745098039216,
      "grad_norm": 0.633812665939331,
      "learning_rate": 1.1190875675987356e-05,
      "loss": 0.0464,
      "num_input_tokens_seen": 16080384,
      "step": 1715
    },
    {
      "epoch": 0.6882753101240496,
      "grad_norm": 0.7012965679168701,
      "learning_rate": 1.1060154982857007e-05,
      "loss": 0.0259,
      "num_input_tokens_seen": 16125568,
      "step": 1720
    },
    {
      "epoch": 0.6902761104441777,
      "grad_norm": 0.3173567056655884,
      "learning_rate": 1.0929985051458908e-05,
      "loss": 0.0308,
      "num_input_tokens_seen": 16175168,
      "step": 1725
    },
    {
      "epoch": 0.6922769107643058,
      "grad_norm": 0.2677067518234253,
      "learning_rate": 1.0800371024792636e-05,
      "loss": 0.0232,
      "num_input_tokens_seen": 16220352,
      "step": 1730
    },
    {
      "epoch": 0.6942777110844338,
      "grad_norm": 0.6412817239761353,
      "learning_rate": 1.0671318023894012e-05,
      "loss": 0.0356,
      "num_input_tokens_seen": 16267584,
      "step": 1735
    },
    {
      "epoch": 0.6962785114045619,
      "grad_norm": 0.5149063467979431,
      "learning_rate": 1.0542831147632823e-05,
      "loss": 0.0418,
      "num_input_tokens_seen": 16316352,
      "step": 1740
    },
    {
      "epoch": 0.6982793117246898,
      "grad_norm": 0.4981881380081177,
      "learning_rate": 1.0414915472511299e-05,
      "loss": 0.0295,
      "num_input_tokens_seen": 16365696,
      "step": 1745
    },
    {
      "epoch": 0.7002801120448179,
      "grad_norm": 0.4131408929824829,
      "learning_rate": 1.0287576052463593e-05,
      "loss": 0.0348,
      "num_input_tokens_seen": 16412032,
      "step": 1750
    },
    {
      "epoch": 0.7022809123649459,
      "grad_norm": 0.6001386046409607,
      "learning_rate": 1.0160817918656092e-05,
      "loss": 0.0245,
      "num_input_tokens_seen": 16457152,
      "step": 1755
    },
    {
      "epoch": 0.704281712685074,
      "grad_norm": 0.6444912552833557,
      "learning_rate": 1.0034646079288612e-05,
      "loss": 0.028,
      "num_input_tokens_seen": 16502464,
      "step": 1760
    },
    {
      "epoch": 0.7062825130052021,
      "grad_norm": 0.6931713819503784,
      "learning_rate": 9.909065519396557e-06,
      "loss": 0.0255,
      "num_input_tokens_seen": 16550784,
      "step": 1765
    },
    {
      "epoch": 0.7082833133253301,
      "grad_norm": 0.4277893304824829,
      "learning_rate": 9.78408120065392e-06,
      "loss": 0.029,
      "num_input_tokens_seen": 16596416,
      "step": 1770
    },
    {
      "epoch": 0.7102841136454582,
      "grad_norm": 0.43115299940109253,
      "learning_rate": 9.659698061177305e-06,
      "loss": 0.0304,
      "num_input_tokens_seen": 16641984,
      "step": 1775
    },
    {
      "epoch": 0.7122849139655862,
      "grad_norm": 0.4906001091003418,
      "learning_rate": 9.53592101533076e-06,
      "loss": 0.0288,
      "num_input_tokens_seen": 16689152,
      "step": 1780
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.41650331020355225,
      "learning_rate": 9.412754953531663e-06,
      "loss": 0.0373,
      "num_input_tokens_seen": 16737664,
      "step": 1785
    },
    {
      "epoch": 0.7162865146058424,
      "grad_norm": 0.48679959774017334,
      "learning_rate": 9.29020474205746e-06,
      "loss": 0.0292,
      "num_input_tokens_seen": 16782144,
      "step": 1790
    },
    {
      "epoch": 0.7182873149259704,
      "grad_norm": 0.5420020222663879,
      "learning_rate": 9.16827522285344e-06,
      "loss": 0.0239,
      "num_input_tokens_seen": 16830784,
      "step": 1795
    },
    {
      "epoch": 0.7202881152460985,
      "grad_norm": 0.7758680582046509,
      "learning_rate": 9.046971213341388e-06,
      "loss": 0.0315,
      "num_input_tokens_seen": 16878464,
      "step": 1800
    },
    {
      "epoch": 0.7222889155662265,
      "grad_norm": 0.5774652361869812,
      "learning_rate": 8.926297506229291e-06,
      "loss": 0.0329,
      "num_input_tokens_seen": 16927296,
      "step": 1805
    },
    {
      "epoch": 0.7242897158863545,
      "grad_norm": 0.3792325258255005,
      "learning_rate": 8.806258869321946e-06,
      "loss": 0.0227,
      "num_input_tokens_seen": 16973632,
      "step": 1810
    },
    {
      "epoch": 0.7262905162064826,
      "grad_norm": 0.542339026927948,
      "learning_rate": 8.68686004533259e-06,
      "loss": 0.0306,
      "num_input_tokens_seen": 17022016,
      "step": 1815
    },
    {
      "epoch": 0.7282913165266106,
      "grad_norm": 0.5014091730117798,
      "learning_rate": 8.568105751695532e-06,
      "loss": 0.0316,
      "num_input_tokens_seen": 17068992,
      "step": 1820
    },
    {
      "epoch": 0.7302921168467387,
      "grad_norm": 0.627659261226654,
      "learning_rate": 8.450000680379766e-06,
      "loss": 0.0298,
      "num_input_tokens_seen": 17113536,
      "step": 1825
    },
    {
      "epoch": 0.7322929171668667,
      "grad_norm": 0.3832060396671295,
      "learning_rate": 8.332549497703562e-06,
      "loss": 0.019,
      "num_input_tokens_seen": 17157696,
      "step": 1830
    },
    {
      "epoch": 0.7342937174869948,
      "grad_norm": 0.7424026727676392,
      "learning_rate": 8.215756844150152e-06,
      "loss": 0.0376,
      "num_input_tokens_seen": 17208896,
      "step": 1835
    },
    {
      "epoch": 0.7362945178071229,
      "grad_norm": 0.6628469824790955,
      "learning_rate": 8.09962733418432e-06,
      "loss": 0.0322,
      "num_input_tokens_seen": 17255360,
      "step": 1840
    },
    {
      "epoch": 0.7382953181272509,
      "grad_norm": 0.6944519281387329,
      "learning_rate": 7.984165556070159e-06,
      "loss": 0.0229,
      "num_input_tokens_seen": 17301184,
      "step": 1845
    },
    {
      "epoch": 0.740296118447379,
      "grad_norm": 0.5525171756744385,
      "learning_rate": 7.86937607168971e-06,
      "loss": 0.0224,
      "num_input_tokens_seen": 17349504,
      "step": 1850
    },
    {
      "epoch": 0.742296918767507,
      "grad_norm": 0.3902173340320587,
      "learning_rate": 7.755263416362802e-06,
      "loss": 0.0233,
      "num_input_tokens_seen": 17397312,
      "step": 1855
    },
    {
      "epoch": 0.7442977190876351,
      "grad_norm": 0.5421453714370728,
      "learning_rate": 7.641832098667786e-06,
      "loss": 0.0284,
      "num_input_tokens_seen": 17441472,
      "step": 1860
    },
    {
      "epoch": 0.7462985194077632,
      "grad_norm": 0.7732987403869629,
      "learning_rate": 7.5290866002634765e-06,
      "loss": 0.0422,
      "num_input_tokens_seen": 17487872,
      "step": 1865
    },
    {
      "epoch": 0.7482993197278912,
      "grad_norm": 0.5669375658035278,
      "learning_rate": 7.417031375712008e-06,
      "loss": 0.032,
      "num_input_tokens_seen": 17534464,
      "step": 1870
    },
    {
      "epoch": 0.7503001200480192,
      "grad_norm": 0.43100404739379883,
      "learning_rate": 7.305670852302904e-06,
      "loss": 0.0209,
      "num_input_tokens_seen": 17580288,
      "step": 1875
    },
    {
      "epoch": 0.7523009203681472,
      "grad_norm": 0.5982136130332947,
      "learning_rate": 7.195009429878097e-06,
      "loss": 0.0253,
      "num_input_tokens_seen": 17627264,
      "step": 1880
    },
    {
      "epoch": 0.7543017206882753,
      "grad_norm": 0.37996983528137207,
      "learning_rate": 7.085051480658123e-06,
      "loss": 0.0282,
      "num_input_tokens_seen": 17674816,
      "step": 1885
    },
    {
      "epoch": 0.7563025210084033,
      "grad_norm": 0.22569338977336884,
      "learning_rate": 6.9758013490693855e-06,
      "loss": 0.0215,
      "num_input_tokens_seen": 17723840,
      "step": 1890
    },
    {
      "epoch": 0.7583033213285314,
      "grad_norm": 0.4934655427932739,
      "learning_rate": 6.867263351572465e-06,
      "loss": 0.03,
      "num_input_tokens_seen": 17771392,
      "step": 1895
    },
    {
      "epoch": 0.7603041216486595,
      "grad_norm": 0.6991490721702576,
      "learning_rate": 6.759441776491635e-06,
      "loss": 0.0284,
      "num_input_tokens_seen": 17819456,
      "step": 1900
    },
    {
      "epoch": 0.7623049219687875,
      "grad_norm": 0.5251808166503906,
      "learning_rate": 6.652340883845365e-06,
      "loss": 0.0314,
      "num_input_tokens_seen": 17866368,
      "step": 1905
    },
    {
      "epoch": 0.7643057222889156,
      "grad_norm": 0.9625482559204102,
      "learning_rate": 6.545964905178073e-06,
      "loss": 0.031,
      "num_input_tokens_seen": 17913536,
      "step": 1910
    },
    {
      "epoch": 0.7663065226090436,
      "grad_norm": 0.5984548926353455,
      "learning_rate": 6.440318043392874e-06,
      "loss": 0.0384,
      "num_input_tokens_seen": 17957376,
      "step": 1915
    },
    {
      "epoch": 0.7683073229291717,
      "grad_norm": 0.4109754264354706,
      "learning_rate": 6.335404472585593e-06,
      "loss": 0.028,
      "num_input_tokens_seen": 18005056,
      "step": 1920
    },
    {
      "epoch": 0.7703081232492998,
      "grad_norm": 0.3721906840801239,
      "learning_rate": 6.231228337879769e-06,
      "loss": 0.0345,
      "num_input_tokens_seen": 18050176,
      "step": 1925
    },
    {
      "epoch": 0.7723089235694278,
      "grad_norm": 0.43133440613746643,
      "learning_rate": 6.127793755262964e-06,
      "loss": 0.0318,
      "num_input_tokens_seen": 18099968,
      "step": 1930
    },
    {
      "epoch": 0.7743097238895558,
      "grad_norm": 0.5904744267463684,
      "learning_rate": 6.025104811424062e-06,
      "loss": 0.0296,
      "num_input_tokens_seen": 18148288,
      "step": 1935
    },
    {
      "epoch": 0.7763105242096838,
      "grad_norm": 0.5099133849143982,
      "learning_rate": 5.923165563591857e-06,
      "loss": 0.0299,
      "num_input_tokens_seen": 18196224,
      "step": 1940
    },
    {
      "epoch": 0.7783113245298119,
      "grad_norm": 0.24885186553001404,
      "learning_rate": 5.821980039374747e-06,
      "loss": 0.0208,
      "num_input_tokens_seen": 18243712,
      "step": 1945
    },
    {
      "epoch": 0.78031212484994,
      "grad_norm": 0.5499646663665771,
      "learning_rate": 5.721552236601574e-06,
      "loss": 0.0386,
      "num_input_tokens_seen": 18290688,
      "step": 1950
    },
    {
      "epoch": 0.782312925170068,
      "grad_norm": 0.26188546419143677,
      "learning_rate": 5.621886123163708e-06,
      "loss": 0.0246,
      "num_input_tokens_seen": 18337408,
      "step": 1955
    },
    {
      "epoch": 0.7843137254901961,
      "grad_norm": 0.4013510048389435,
      "learning_rate": 5.522985636858239e-06,
      "loss": 0.022,
      "num_input_tokens_seen": 18384576,
      "step": 1960
    },
    {
      "epoch": 0.7863145258103241,
      "grad_norm": 0.5112025737762451,
      "learning_rate": 5.424854685232436e-06,
      "loss": 0.0334,
      "num_input_tokens_seen": 18431808,
      "step": 1965
    },
    {
      "epoch": 0.7883153261304522,
      "grad_norm": 0.27657797932624817,
      "learning_rate": 5.327497145429314e-06,
      "loss": 0.0183,
      "num_input_tokens_seen": 18478656,
      "step": 1970
    },
    {
      "epoch": 0.7903161264505802,
      "grad_norm": 0.7116694450378418,
      "learning_rate": 5.230916864034497e-06,
      "loss": 0.0276,
      "num_input_tokens_seen": 18526656,
      "step": 1975
    },
    {
      "epoch": 0.7923169267707083,
      "grad_norm": 0.2906137704849243,
      "learning_rate": 5.135117656924187e-06,
      "loss": 0.0188,
      "num_input_tokens_seen": 18576128,
      "step": 1980
    },
    {
      "epoch": 0.7943177270908364,
      "grad_norm": 0.4715001881122589,
      "learning_rate": 5.040103309114463e-06,
      "loss": 0.0224,
      "num_input_tokens_seen": 18626816,
      "step": 1985
    },
    {
      "epoch": 0.7963185274109644,
      "grad_norm": 0.3023926317691803,
      "learning_rate": 4.94587757461166e-06,
      "loss": 0.0211,
      "num_input_tokens_seen": 18673472,
      "step": 1990
    },
    {
      "epoch": 0.7983193277310925,
      "grad_norm": 0.3166676461696625,
      "learning_rate": 4.852444176264129e-06,
      "loss": 0.0258,
      "num_input_tokens_seen": 18719104,
      "step": 1995
    },
    {
      "epoch": 0.8003201280512204,
      "grad_norm": 0.8794540762901306,
      "learning_rate": 4.759806805615074e-06,
      "loss": 0.0343,
      "num_input_tokens_seen": 18764352,
      "step": 2000
    },
    {
      "epoch": 0.8023209283713485,
      "grad_norm": 0.7100375890731812,
      "learning_rate": 4.667969122756755e-06,
      "loss": 0.0286,
      "num_input_tokens_seen": 18811392,
      "step": 2005
    },
    {
      "epoch": 0.8043217286914766,
      "grad_norm": 0.7670468091964722,
      "learning_rate": 4.57693475618583e-06,
      "loss": 0.0274,
      "num_input_tokens_seen": 18860352,
      "step": 2010
    },
    {
      "epoch": 0.8063225290116046,
      "grad_norm": 0.5464458465576172,
      "learning_rate": 4.486707302660059e-06,
      "loss": 0.0279,
      "num_input_tokens_seen": 18909120,
      "step": 2015
    },
    {
      "epoch": 0.8083233293317327,
      "grad_norm": 0.5225525498390198,
      "learning_rate": 4.397290327056114e-06,
      "loss": 0.0311,
      "num_input_tokens_seen": 18956032,
      "step": 2020
    },
    {
      "epoch": 0.8103241296518607,
      "grad_norm": 0.3628346920013428,
      "learning_rate": 4.308687362228814e-06,
      "loss": 0.0325,
      "num_input_tokens_seen": 19000256,
      "step": 2025
    },
    {
      "epoch": 0.8123249299719888,
      "grad_norm": 0.5620947480201721,
      "learning_rate": 4.220901908871469e-06,
      "loss": 0.0296,
      "num_input_tokens_seen": 19047040,
      "step": 2030
    },
    {
      "epoch": 0.8143257302921169,
      "grad_norm": 0.6291869878768921,
      "learning_rate": 4.133937435377624e-06,
      "loss": 0.0303,
      "num_input_tokens_seen": 19094336,
      "step": 2035
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.27330756187438965,
      "learning_rate": 4.047797377703985e-06,
      "loss": 0.0298,
      "num_input_tokens_seen": 19139008,
      "step": 2040
    },
    {
      "epoch": 0.818327330932373,
      "grad_norm": 0.38108041882514954,
      "learning_rate": 3.962485139234695e-06,
      "loss": 0.027,
      "num_input_tokens_seen": 19184064,
      "step": 2045
    },
    {
      "epoch": 0.820328131252501,
      "grad_norm": 0.2633708417415619,
      "learning_rate": 3.878004090646836e-06,
      "loss": 0.0193,
      "num_input_tokens_seen": 19233152,
      "step": 2050
    },
    {
      "epoch": 0.8223289315726291,
      "grad_norm": 0.7602033615112305,
      "learning_rate": 3.794357569777282e-06,
      "loss": 0.033,
      "num_input_tokens_seen": 19278848,
      "step": 2055
    },
    {
      "epoch": 0.8243297318927572,
      "grad_norm": 0.397424578666687,
      "learning_rate": 3.7115488814908117e-06,
      "loss": 0.0279,
      "num_input_tokens_seen": 19324864,
      "step": 2060
    },
    {
      "epoch": 0.8263305322128851,
      "grad_norm": 0.44060972332954407,
      "learning_rate": 3.6295812975495196e-06,
      "loss": 0.0283,
      "num_input_tokens_seen": 19372352,
      "step": 2065
    },
    {
      "epoch": 0.8283313325330132,
      "grad_norm": 0.7760177254676819,
      "learning_rate": 3.5484580564835668e-06,
      "loss": 0.0284,
      "num_input_tokens_seen": 19418432,
      "step": 2070
    },
    {
      "epoch": 0.8303321328531412,
      "grad_norm": 0.5415899753570557,
      "learning_rate": 3.468182363463213e-06,
      "loss": 0.0317,
      "num_input_tokens_seen": 19463488,
      "step": 2075
    },
    {
      "epoch": 0.8323329331732693,
      "grad_norm": 0.30139076709747314,
      "learning_rate": 3.3887573901722093e-06,
      "loss": 0.021,
      "num_input_tokens_seen": 19510208,
      "step": 2080
    },
    {
      "epoch": 0.8343337334933973,
      "grad_norm": 0.7119060158729553,
      "learning_rate": 3.3101862746824363e-06,
      "loss": 0.0354,
      "num_input_tokens_seen": 19556480,
      "step": 2085
    },
    {
      "epoch": 0.8363345338135254,
      "grad_norm": 0.2610571086406708,
      "learning_rate": 3.232472121329977e-06,
      "loss": 0.0234,
      "num_input_tokens_seen": 19603264,
      "step": 2090
    },
    {
      "epoch": 0.8383353341336535,
      "grad_norm": 0.3571932017803192,
      "learning_rate": 3.1556180005924085e-06,
      "loss": 0.0277,
      "num_input_tokens_seen": 19651840,
      "step": 2095
    },
    {
      "epoch": 0.8403361344537815,
      "grad_norm": 0.6296040415763855,
      "learning_rate": 3.0796269489675344e-06,
      "loss": 0.0481,
      "num_input_tokens_seen": 19701568,
      "step": 2100
    },
    {
      "epoch": 0.8423369347739096,
      "grad_norm": 0.4878975749015808,
      "learning_rate": 3.0045019688533795e-06,
      "loss": 0.0248,
      "num_input_tokens_seen": 19749952,
      "step": 2105
    },
    {
      "epoch": 0.8443377350940376,
      "grad_norm": 0.37973901629447937,
      "learning_rate": 2.9302460284295952e-06,
      "loss": 0.0207,
      "num_input_tokens_seen": 19795584,
      "step": 2110
    },
    {
      "epoch": 0.8463385354141657,
      "grad_norm": 0.3894746005535126,
      "learning_rate": 2.856862061540147e-06,
      "loss": 0.0333,
      "num_input_tokens_seen": 19842816,
      "step": 2115
    },
    {
      "epoch": 0.8483393357342938,
      "grad_norm": 0.6030480265617371,
      "learning_rate": 2.784352967577447e-06,
      "loss": 0.0363,
      "num_input_tokens_seen": 19888384,
      "step": 2120
    },
    {
      "epoch": 0.8503401360544217,
      "grad_norm": 0.4768061637878418,
      "learning_rate": 2.7127216113677635e-06,
      "loss": 0.0292,
      "num_input_tokens_seen": 19938048,
      "step": 2125
    },
    {
      "epoch": 0.8523409363745498,
      "grad_norm": 0.5876083970069885,
      "learning_rate": 2.6419708230580374e-06,
      "loss": 0.0233,
      "num_input_tokens_seen": 19986112,
      "step": 2130
    },
    {
      "epoch": 0.8543417366946778,
      "grad_norm": 0.5123450756072998,
      "learning_rate": 2.572103398004086e-06,
      "loss": 0.0231,
      "num_input_tokens_seen": 20033856,
      "step": 2135
    },
    {
      "epoch": 0.8563425370148059,
      "grad_norm": 0.5924628376960754,
      "learning_rate": 2.503122096660121e-06,
      "loss": 0.0265,
      "num_input_tokens_seen": 20079872,
      "step": 2140
    },
    {
      "epoch": 0.858343337334934,
      "grad_norm": 0.3032357394695282,
      "learning_rate": 2.43502964446973e-06,
      "loss": 0.0345,
      "num_input_tokens_seen": 20128512,
      "step": 2145
    },
    {
      "epoch": 0.860344137655062,
      "grad_norm": 0.6530863046646118,
      "learning_rate": 2.3678287317581425e-06,
      "loss": 0.03,
      "num_input_tokens_seen": 20178560,
      "step": 2150
    },
    {
      "epoch": 0.8623449379751901,
      "grad_norm": 0.18704676628112793,
      "learning_rate": 2.301522013625984e-06,
      "loss": 0.0204,
      "num_input_tokens_seen": 20227136,
      "step": 2155
    },
    {
      "epoch": 0.8643457382953181,
      "grad_norm": 0.3851707875728607,
      "learning_rate": 2.236112109844335e-06,
      "loss": 0.0212,
      "num_input_tokens_seen": 20276672,
      "step": 2160
    },
    {
      "epoch": 0.8663465386154462,
      "grad_norm": 0.4089384377002716,
      "learning_rate": 2.1716016047512555e-06,
      "loss": 0.0309,
      "num_input_tokens_seen": 20326080,
      "step": 2165
    },
    {
      "epoch": 0.8683473389355743,
      "grad_norm": 0.4709281325340271,
      "learning_rate": 2.107993047149645e-06,
      "loss": 0.0273,
      "num_input_tokens_seen": 20376960,
      "step": 2170
    },
    {
      "epoch": 0.8703481392557023,
      "grad_norm": 0.5483434200286865,
      "learning_rate": 2.0452889502065753e-06,
      "loss": 0.0336,
      "num_input_tokens_seen": 20423872,
      "step": 2175
    },
    {
      "epoch": 0.8723489395758304,
      "grad_norm": 0.6315237283706665,
      "learning_rate": 1.9834917913539612e-06,
      "loss": 0.0264,
      "num_input_tokens_seen": 20469184,
      "step": 2180
    },
    {
      "epoch": 0.8743497398959584,
      "grad_norm": 0.2926303446292877,
      "learning_rate": 1.922604012190715e-06,
      "loss": 0.0158,
      "num_input_tokens_seen": 20514304,
      "step": 2185
    },
    {
      "epoch": 0.8763505402160864,
      "grad_norm": 0.829858660697937,
      "learning_rate": 1.8626280183862366e-06,
      "loss": 0.0315,
      "num_input_tokens_seen": 20561472,
      "step": 2190
    },
    {
      "epoch": 0.8783513405362144,
      "grad_norm": 0.3657209873199463,
      "learning_rate": 1.8035661795853976e-06,
      "loss": 0.0324,
      "num_input_tokens_seen": 20610176,
      "step": 2195
    },
    {
      "epoch": 0.8803521408563425,
      "grad_norm": 0.45679229497909546,
      "learning_rate": 1.7454208293149032e-06,
      "loss": 0.0216,
      "num_input_tokens_seen": 20658880,
      "step": 2200
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.597928524017334,
      "learning_rate": 1.6881942648911076e-06,
      "loss": 0.0191,
      "num_input_tokens_seen": 20706688,
      "step": 2205
    },
    {
      "epoch": 0.8843537414965986,
      "grad_norm": 0.4616369307041168,
      "learning_rate": 1.6318887473292243e-06,
      "loss": 0.0223,
      "num_input_tokens_seen": 20752192,
      "step": 2210
    },
    {
      "epoch": 0.8863545418167267,
      "grad_norm": 0.6525319218635559,
      "learning_rate": 1.5765065012540214e-06,
      "loss": 0.0332,
      "num_input_tokens_seen": 20800640,
      "step": 2215
    },
    {
      "epoch": 0.8883553421368547,
      "grad_norm": 0.4937126040458679,
      "learning_rate": 1.522049714811899e-06,
      "loss": 0.0342,
      "num_input_tokens_seen": 20849472,
      "step": 2220
    },
    {
      "epoch": 0.8903561424569828,
      "grad_norm": 0.5141566395759583,
      "learning_rate": 1.4685205395844587e-06,
      "loss": 0.0346,
      "num_input_tokens_seen": 20897344,
      "step": 2225
    },
    {
      "epoch": 0.8923569427771109,
      "grad_norm": 0.49359285831451416,
      "learning_rate": 1.4159210905034858e-06,
      "loss": 0.0256,
      "num_input_tokens_seen": 20942592,
      "step": 2230
    },
    {
      "epoch": 0.8943577430972389,
      "grad_norm": 0.5710551738739014,
      "learning_rate": 1.36425344576738e-06,
      "loss": 0.0284,
      "num_input_tokens_seen": 20992512,
      "step": 2235
    },
    {
      "epoch": 0.896358543417367,
      "grad_norm": 0.4512149393558502,
      "learning_rate": 1.3135196467590704e-06,
      "loss": 0.0369,
      "num_input_tokens_seen": 21039232,
      "step": 2240
    },
    {
      "epoch": 0.898359343737495,
      "grad_norm": 0.33637315034866333,
      "learning_rate": 1.2637216979653227e-06,
      "loss": 0.0215,
      "num_input_tokens_seen": 21084672,
      "step": 2245
    },
    {
      "epoch": 0.9003601440576231,
      "grad_norm": 0.8777936697006226,
      "learning_rate": 1.2148615668975876e-06,
      "loss": 0.0354,
      "num_input_tokens_seen": 21130624,
      "step": 2250
    },
    {
      "epoch": 0.902360944377751,
      "grad_norm": 0.40554121136665344,
      "learning_rate": 1.166941184014228e-06,
      "loss": 0.0301,
      "num_input_tokens_seen": 21175488,
      "step": 2255
    },
    {
      "epoch": 0.9043617446978791,
      "grad_norm": 0.13954389095306396,
      "learning_rate": 1.1199624426442596e-06,
      "loss": 0.0261,
      "num_input_tokens_seen": 21222464,
      "step": 2260
    },
    {
      "epoch": 0.9063625450180072,
      "grad_norm": 0.16728809475898743,
      "learning_rate": 1.0739271989125471e-06,
      "loss": 0.0285,
      "num_input_tokens_seen": 21268864,
      "step": 2265
    },
    {
      "epoch": 0.9083633453381352,
      "grad_norm": 0.4615810215473175,
      "learning_rate": 1.0288372716664745e-06,
      "loss": 0.0317,
      "num_input_tokens_seen": 21315712,
      "step": 2270
    },
    {
      "epoch": 0.9103641456582633,
      "grad_norm": 0.5006642937660217,
      "learning_rate": 9.846944424040688e-07,
      "loss": 0.034,
      "num_input_tokens_seen": 21359808,
      "step": 2275
    },
    {
      "epoch": 0.9123649459783914,
      "grad_norm": 0.422631174325943,
      "learning_rate": 9.41500455203631e-07,
      "loss": 0.0233,
      "num_input_tokens_seen": 21408064,
      "step": 2280
    },
    {
      "epoch": 0.9143657462985194,
      "grad_norm": 0.5387400984764099,
      "learning_rate": 8.992570166547976e-07,
      "loss": 0.0299,
      "num_input_tokens_seen": 21454208,
      "step": 2285
    },
    {
      "epoch": 0.9163665466186475,
      "grad_norm": 0.6335134506225586,
      "learning_rate": 8.579657957911575e-07,
      "loss": 0.025,
      "num_input_tokens_seen": 21502400,
      "step": 2290
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 0.7379043102264404,
      "learning_rate": 8.176284240242638e-07,
      "loss": 0.0262,
      "num_input_tokens_seen": 21549824,
      "step": 2295
    },
    {
      "epoch": 0.9203681472589036,
      "grad_norm": 0.6300957798957825,
      "learning_rate": 7.782464950792128e-07,
      "loss": 0.0298,
      "num_input_tokens_seen": 21598592,
      "step": 2300
    },
    {
      "epoch": 0.9223689475790317,
      "grad_norm": 0.5425515174865723,
      "learning_rate": 7.398215649316503e-07,
      "loss": 0.0273,
      "num_input_tokens_seen": 21645184,
      "step": 2305
    },
    {
      "epoch": 0.9243697478991597,
      "grad_norm": 0.7076255679130554,
      "learning_rate": 7.02355151746309e-07,
      "loss": 0.0338,
      "num_input_tokens_seen": 21694400,
      "step": 2310
    },
    {
      "epoch": 0.9263705482192878,
      "grad_norm": 0.7007192969322205,
      "learning_rate": 6.658487358170234e-07,
      "loss": 0.0313,
      "num_input_tokens_seen": 21743488,
      "step": 2315
    },
    {
      "epoch": 0.9283713485394157,
      "grad_norm": 0.9792742133140564,
      "learning_rate": 6.303037595082467e-07,
      "loss": 0.0382,
      "num_input_tokens_seen": 21790464,
      "step": 2320
    },
    {
      "epoch": 0.9303721488595438,
      "grad_norm": 0.4383678436279297,
      "learning_rate": 5.957216271980509e-07,
      "loss": 0.0235,
      "num_input_tokens_seen": 21837120,
      "step": 2325
    },
    {
      "epoch": 0.9323729491796718,
      "grad_norm": 0.3045094609260559,
      "learning_rate": 5.621037052226497e-07,
      "loss": 0.0279,
      "num_input_tokens_seen": 21884544,
      "step": 2330
    },
    {
      "epoch": 0.9343737494997999,
      "grad_norm": 0.41239699721336365,
      "learning_rate": 5.294513218224218e-07,
      "loss": 0.0224,
      "num_input_tokens_seen": 21930368,
      "step": 2335
    },
    {
      "epoch": 0.936374549819928,
      "grad_norm": 0.38583436608314514,
      "learning_rate": 4.977657670894115e-07,
      "loss": 0.0193,
      "num_input_tokens_seen": 21976960,
      "step": 2340
    },
    {
      "epoch": 0.938375350140056,
      "grad_norm": 0.5222314596176147,
      "learning_rate": 4.6704829291638053e-07,
      "loss": 0.0324,
      "num_input_tokens_seen": 22021312,
      "step": 2345
    },
    {
      "epoch": 0.9403761504601841,
      "grad_norm": 0.692422091960907,
      "learning_rate": 4.3730011294732807e-07,
      "loss": 0.0389,
      "num_input_tokens_seen": 22067968,
      "step": 2350
    },
    {
      "epoch": 0.9423769507803121,
      "grad_norm": 0.6329745054244995,
      "learning_rate": 4.0852240252955143e-07,
      "loss": 0.0162,
      "num_input_tokens_seen": 22115392,
      "step": 2355
    },
    {
      "epoch": 0.9443777511004402,
      "grad_norm": 0.5490590333938599,
      "learning_rate": 3.807162986671997e-07,
      "loss": 0.0261,
      "num_input_tokens_seen": 22163136,
      "step": 2360
    },
    {
      "epoch": 0.9463785514205683,
      "grad_norm": 0.4594327211380005,
      "learning_rate": 3.5388289997635436e-07,
      "loss": 0.0281,
      "num_input_tokens_seen": 22213888,
      "step": 2365
    },
    {
      "epoch": 0.9483793517406963,
      "grad_norm": 0.6147088408470154,
      "learning_rate": 3.2802326664162495e-07,
      "loss": 0.0424,
      "num_input_tokens_seen": 22259008,
      "step": 2370
    },
    {
      "epoch": 0.9503801520608244,
      "grad_norm": 0.5780505537986755,
      "learning_rate": 3.03138420374266e-07,
      "loss": 0.0359,
      "num_input_tokens_seen": 22308096,
      "step": 2375
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.1579606831073761,
      "learning_rate": 2.7922934437178695e-07,
      "loss": 0.0232,
      "num_input_tokens_seen": 22352320,
      "step": 2380
    },
    {
      "epoch": 0.9543817527010804,
      "grad_norm": 0.23244419693946838,
      "learning_rate": 2.5629698327913897e-07,
      "loss": 0.0204,
      "num_input_tokens_seen": 22398336,
      "step": 2385
    },
    {
      "epoch": 0.9563825530212084,
      "grad_norm": 0.561518132686615,
      "learning_rate": 2.3434224315136143e-07,
      "loss": 0.0297,
      "num_input_tokens_seen": 22446784,
      "step": 2390
    },
    {
      "epoch": 0.9583833533413365,
      "grad_norm": 0.7579525113105774,
      "learning_rate": 2.1336599141781322e-07,
      "loss": 0.0498,
      "num_input_tokens_seen": 22491904,
      "step": 2395
    },
    {
      "epoch": 0.9603841536614646,
      "grad_norm": 0.2699304223060608,
      "learning_rate": 1.9336905684786688e-07,
      "loss": 0.0262,
      "num_input_tokens_seen": 22542720,
      "step": 2400
    },
    {
      "epoch": 0.9623849539815926,
      "grad_norm": 0.44288599491119385,
      "learning_rate": 1.7435222951819875e-07,
      "loss": 0.0223,
      "num_input_tokens_seen": 22591232,
      "step": 2405
    },
    {
      "epoch": 0.9643857543017207,
      "grad_norm": 0.7039044499397278,
      "learning_rate": 1.5631626078154716e-07,
      "loss": 0.0368,
      "num_input_tokens_seen": 22638592,
      "step": 2410
    },
    {
      "epoch": 0.9663865546218487,
      "grad_norm": 0.6114466786384583,
      "learning_rate": 1.3926186323703905e-07,
      "loss": 0.032,
      "num_input_tokens_seen": 22683584,
      "step": 2415
    },
    {
      "epoch": 0.9683873549419768,
      "grad_norm": 0.4946146607398987,
      "learning_rate": 1.2318971070203466e-07,
      "loss": 0.0317,
      "num_input_tokens_seen": 22730560,
      "step": 2420
    },
    {
      "epoch": 0.9703881552621049,
      "grad_norm": 0.6754703521728516,
      "learning_rate": 1.0810043818549332e-07,
      "loss": 0.0249,
      "num_input_tokens_seen": 22775936,
      "step": 2425
    },
    {
      "epoch": 0.9723889555822329,
      "grad_norm": 0.45587339997291565,
      "learning_rate": 9.39946418629073e-08,
      "loss": 0.027,
      "num_input_tokens_seen": 22824128,
      "step": 2430
    },
    {
      "epoch": 0.974389755902361,
      "grad_norm": 0.22095923125743866,
      "learning_rate": 8.087287905272356e-08,
      "loss": 0.0163,
      "num_input_tokens_seen": 22872448,
      "step": 2435
    },
    {
      "epoch": 0.976390556222489,
      "grad_norm": 0.48641544580459595,
      "learning_rate": 6.873566819433907e-08,
      "loss": 0.0395,
      "num_input_tokens_seen": 22918080,
      "step": 2440
    },
    {
      "epoch": 0.978391356542617,
      "grad_norm": 0.49065059423446655,
      "learning_rate": 5.758348882760611e-08,
      "loss": 0.0381,
      "num_input_tokens_seen": 22961664,
      "step": 2445
    },
    {
      "epoch": 0.9803921568627451,
      "grad_norm": 0.6738808751106262,
      "learning_rate": 4.741678157389739e-08,
      "loss": 0.0354,
      "num_input_tokens_seen": 23007552,
      "step": 2450
    },
    {
      "epoch": 0.9823929571828731,
      "grad_norm": 0.473112016916275,
      "learning_rate": 3.823594811869224e-08,
      "loss": 0.0431,
      "num_input_tokens_seen": 23056320,
      "step": 2455
    },
    {
      "epoch": 0.9843937575030012,
      "grad_norm": 0.5058896541595459,
      "learning_rate": 3.004135119570317e-08,
      "loss": 0.0404,
      "num_input_tokens_seen": 23104448,
      "step": 2460
    },
    {
      "epoch": 0.9863945578231292,
      "grad_norm": 0.54194575548172,
      "learning_rate": 2.2833314572542895e-08,
      "loss": 0.0227,
      "num_input_tokens_seen": 23152448,
      "step": 2465
    },
    {
      "epoch": 0.9883953581432573,
      "grad_norm": 0.5478609204292297,
      "learning_rate": 1.6612123037945683e-08,
      "loss": 0.0225,
      "num_input_tokens_seen": 23198976,
      "step": 2470
    },
    {
      "epoch": 0.9903961584633854,
      "grad_norm": 0.52669757604599,
      "learning_rate": 1.137802239049579e-08,
      "loss": 0.0267,
      "num_input_tokens_seen": 23246656,
      "step": 2475
    },
    {
      "epoch": 0.9923969587835134,
      "grad_norm": 0.27300211787223816,
      "learning_rate": 7.131219428929692e-09,
      "loss": 0.0228,
      "num_input_tokens_seen": 23292288,
      "step": 2480
    },
    {
      "epoch": 0.9943977591036415,
      "grad_norm": 0.31343570351600647,
      "learning_rate": 3.871881943962041e-09,
      "loss": 0.0239,
      "num_input_tokens_seen": 23340736,
      "step": 2485
    },
    {
      "epoch": 0.9963985594237695,
      "grad_norm": 0.4659945070743561,
      "learning_rate": 1.600138711660426e-09,
      "loss": 0.0225,
      "num_input_tokens_seen": 23390976,
      "step": 2490
    },
    {
      "epoch": 0.9983993597438976,
      "grad_norm": 0.3581176996231079,
      "learning_rate": 3.1607948834111447e-10,
      "loss": 0.0157,
      "num_input_tokens_seen": 23436992,
      "step": 2495
    },
    {
      "epoch": 1.0,
      "num_input_tokens_seen": 23472928,
      "step": 2499,
      "total_flos": 3.9869767966949376e+17,
      "train_loss": 0.04613605415036364,
      "train_runtime": 124849.9299,
      "train_samples_per_second": 1.281,
      "train_steps_per_second": 0.02
    }
  ],
  "logging_steps": 5,
  "max_steps": 2499,
  "num_input_tokens_seen": 23472928,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.9869767966949376e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
